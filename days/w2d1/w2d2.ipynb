{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run w2d1.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "from typing import Callable, Dict, Optional, List, Tuple\n",
    "\n",
    "import torch as t\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch import einsum\n",
    "from einops import rearrange, reduce, repeat\n",
    "import bert_tests\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "# print(tokenizer(\"hello what's up\"))\n",
    "# uncased_tokenizer = transformers.AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "# print(uncased_tokenizer([\"hello what's up\"]))\n",
    "# coded = uncased_tokenizer([\"hello what's up\"])\n",
    "# uncased_tokenizer.batch_decode(coded['input_ids'])\n",
    "# tokenizer.batch_decode(coded['input_ids'])\n",
    "# uncased_tokenizer.batch_decode(coded['input_ids'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "my_bert, pretrained_bert = load_pretrained_bert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The fish loves to eat [MASK].\n",
      "it fish them meat food eggs honey insects too rice everything water vegetables this fruit apples him there again here\n",
      "tensor([0.1738, 0.0980, 0.0947, 0.0410, 0.0336, 0.0251, 0.0134, 0.0130, 0.0126,\n",
      "        0.0119, 0.0092, 0.0090, 0.0088, 0.0083, 0.0072, 0.0069, 0.0063, 0.0060,\n",
      "        0.0058, 0.0054], grad_fn=<IndexBackward0>)\n",
      "\n",
      "The fish loves to eat [MASK]\n",
      ". ;!?..., : | and \" but - so । because as [UNK]') with\n",
      "tensor([9.4125e-01, 4.6098e-02, 1.1822e-02, 4.5820e-04, 1.2235e-04, 5.4506e-05,\n",
      "        3.6213e-05, 1.6483e-05, 1.2279e-05, 9.2127e-06, 6.5461e-06, 4.6536e-06,\n",
      "        3.4753e-06, 3.3669e-06, 2.9931e-06, 2.4598e-06, 1.9791e-06, 1.7764e-06,\n",
      "        1.3952e-06, 1.0635e-06], grad_fn=<IndexBackward0>)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def feed_bert(model: nn.Module, text: str, tokenizer, top_k: int = 10):\n",
    "    input_ids: List[int] = tokenizer(text)[\"input_ids\"]\n",
    "    mask_idxs = [idx for idx, token in enumerate(input_ids) if token == 103]\n",
    "\n",
    "    all_logits = model(t.tensor([input_ids], dtype=t.long))[0]\n",
    "\n",
    "    print(text)\n",
    "    for mask_idx in mask_idxs:\n",
    "        logits = all_logits[mask_idx]\n",
    "        probs = t.softmax(logits, dim=0)\n",
    "\n",
    "        top_logit_idxs = t.argsort(logits, descending=True)[:top_k]\n",
    "        top_logit_words = tokenizer.decode(top_logit_idxs)\n",
    "\n",
    "        print(top_logit_words)\n",
    "        print(probs[top_logit_idxs])\n",
    "        print()\n",
    "\n",
    "my_bert.eval()\n",
    "feed_bert(my_bert, \"The fish loves to eat [MASK].\", tokenizer, top_k=20)\n",
    "feed_bert(my_bert, \"The fish loves to eat [MASK]\", tokenizer, top_k=20)\n",
    "#feed_bert(my_bert, \"The vegetarian fish loves to eat [MASK].\", tokenizer, top_k=20)\n",
    "#feed_bert(my_bert, \"The meat-eating fish loves to eat [MASK].\", tokenizer, top_k=20)\n",
    "#feed_bert(my_bert, \"The tiny fish loves to eat [MASK].\", tokenizer, top_k=20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert MATCH!!!!!!!!\n",
      " SHAPE (1, 4, 28996) MEAN: 0.003031 STD: 0.5765 VALS [-0.5742 -0.432 0.1186 -0.7165 -0.5261 0.4967 1.223 0.3165 -0.3247 -0.5716...]\n",
      "bert MATCH!!!!!!!!\n",
      " SHAPE (1, 2) MEAN: 0.09479 STD: 1.411 VALS [-0.903 1.093]\n"
     ]
    }
   ],
   "source": [
    "bert_tests.test_bert_classification(Bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_imdb_collate_fn(\n",
    "    max_seq_length: int,\n",
    "    tokenizer: transformers.AutoTokenizer,\n",
    "    device: str,\n",
    "):\n",
    "    def fn(raw_xs: List[Tuple[str, str]]) -> Tuple[t.Tensor, t.Tensor]:\n",
    "        labels: Tuple[str, ...]\n",
    "        texts: Tuple[str, ...]\n",
    "        labels, texts = zip(*raw_xs)\n",
    "\n",
    "        xs = t.tensor(\n",
    "            tokenizer(\n",
    "                list(texts),\n",
    "                padding=\"longest\",\n",
    "                max_length=max_seq_length,\n",
    "                truncation=True,\n",
    "            )['input_ids'],\n",
    "            dtype=t.long,\n",
    "            device=device,\n",
    "        )\n",
    "\n",
    "        ys = t.tensor([int(l == \"pos\") for l in labels], dtype=t.long, device=device)\n",
    "\n",
    "        return xs, ys\n",
    "\n",
    "    return fn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torchtext\n",
    "\n",
    "data_train, data_test = torchtext.datasets.IMDB(root='.data', split=('train', 'test'))\n",
    "data_train = list(data_train)\n",
    "data_test = list(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "device = \"cuda\"\n",
    "collate_fn = get_imdb_collate_fn(512, tokenizer, device)\n",
    "\n",
    "dl_train_small = DataLoader(\n",
    "    random.sample(data_train, k=16),\n",
    "    batch_size=16,\n",
    "    collate_fn=collate_fn,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "dl_train = DataLoader(\n",
    "    data_train,\n",
    "    batch_size=8,\n",
    "    collate_fn=collate_fn,\n",
    "    shuffle=True,\n",
    "    # num_workers=0,\n",
    "    # pin_memory=True,\n",
    ")\n",
    "\n",
    "dl_test_small = DataLoader(\n",
    "    random.sample(data_test, k=256),\n",
    "    batch_size=16,\n",
    "    collate_fn=collate_fn,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "dl_test = DataLoader(\n",
    "    data_test,\n",
    "    batch_size=2,\n",
    "    collate_fn=collate_fn,\n",
    "    shuffle=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss=0.012,acc=0.9375: : 3125it [16:13,  3.21it/s]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss=0.34,acc=0.90234375: : 1245it [06:28,  3.20it/s]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-90a0525f9e4c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfinetune_bert_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_bert\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdl_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdl_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdl_test\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdl_test_small\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-90a0525f9e4c>\u001b[0m in \u001b[0;36mfinetune_bert_epoch\u001b[0;34m(model, dl_train, dl_test)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mpbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_description\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'loss={loss.item():.2},acc={get_accuracy(model, dl_test)}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    131\u001b[0m                     \u001b[0mstate_steps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'step'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m             F.adam(params_with_grad,\n\u001b[0m\u001b[1;32m    134\u001b[0m                    \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m                    \u001b[0mexp_avgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/optim/_functional.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax_exp_avg_sqs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m             \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch import optim\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "def get_accuracy(model: nn.Module, dl: DataLoader) -> float:\n",
    "\n",
    "    num_correct: int = 0\n",
    "    num_total: int = 0\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    pbar = tqdm(dl, disable=True)\n",
    "    for x, y in pbar:\n",
    "        _, out = model(x)\n",
    "        preds = t.argmax(out, dim=-1)\n",
    "\n",
    "        num_correct += (preds == y).sum()\n",
    "        num_total += len(y)\n",
    "        pbar.set_description(f'acc={num_correct / num_total:.2}')\n",
    "\n",
    "    return num_correct / num_total\n",
    "\n",
    "def finetune_bert_epoch(model: nn.Module, dl_train: DataLoader, dl_test: DataLoader) -> nn.Module:\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-5)  # broken?\n",
    "    pbar = tqdm(enumerate(dl_train))\n",
    "    for i, (x, y) in pbar:\n",
    "        optimizer.zero_grad()\n",
    "        _, out = model(x)\n",
    "        loss = F.cross_entropy(input=out, target=y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i % 10 == 0:\n",
    "            pbar.set_description(f'loss={loss.item():.2},acc={get_accuracy(model, dl_test)}')\n",
    "            model.train()\n",
    "\n",
    "    return model\n",
    "\n",
    "my_bert, _ = load_pretrained_bert(num_classes=2)\n",
    "#for i, (name, p) in enumerate(my_bert.named_parameters()):\n",
    "#    print(name)\n",
    "#    p.cuda()\n",
    "\n",
    "my_bert.cuda()\n",
    "my_bert.train()\n",
    "\n",
    "# print(get_accuracy(my_bert, dl_test_small))\n",
    "import gc\n",
    "gc.collect()\n",
    "t.cuda.empty_cache()\n",
    "epochs = 100\n",
    "for i in range(epochs):\n",
    "    print(i)\n",
    "    model = finetune_bert_epoch(my_bert, dl_train=dl_train, dl_test=dl_test_small)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned ON\n"
     ]
    }
   ],
   "source": [
    "%pdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training from Scratch on Masked Language Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train, data_test = torchtext.datasets.WikiText2(root='.data', split=('train', 'test'))\n",
    "\n",
    "def wiki_include(text: str) -> bool:\n",
    "    return text.split(\"text\")\n",
    "\n",
    "data_train = list(data_train)\n",
    "data_test = list(data_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wiki_collate_fn(\n",
    "    max_seq_length: int,\n",
    "    tokenizer: transformers.PreTrainedTokenizerBase,\n",
    "    pred_frac: float,\n",
    "    mask_frac: float,\n",
    "    random_frac: float,\n",
    "    device: str,\n",
    "):\n",
    "    assert 0 <= pred_frac <= 1 and 0 <= mask_frac <= 1 and 0 <= random_frac <= 1\n",
    "    assert 0 <= mask_frac + random_frac <= 1\n",
    "    unchanged_frac = 1 - mask_frac - random_frac\n",
    "\n",
    "    def fn(texts: List[str]) -> Tuple[t.Tensor, t.Tensor]:\n",
    "        # TODO: Sample random substring of texts to have more data diversity?\n",
    "        xs = t.tensor(\n",
    "            tokenizer(\n",
    "                list(texts),\n",
    "                padding=\"longest\",\n",
    "                max_length=max_seq_length,\n",
    "                truncation=True,\n",
    "            )[\"input_ids\"],\n",
    "            dtype=t.long,\n",
    "            device=device,\n",
    "        )\n",
    "\n",
    "        pred_mask = (t.rand_like(xs, dtype=t.float) < pred_frac) & (\n",
    "            (xs != tokenizer.pad_token_id) & (xs != tokenizer.cls_token_id) &\n",
    "            (xs != tokenizer.eos_token_id) & (xs != tokenizer.sep_token_id)\n",
    "        )\n",
    "        ys = t.masked_select(xs, pred_mask)\n",
    "\n",
    "        r = t.rand_like(xs, dtype=t.float)\n",
    "        mask_mask = r < mask_frac\n",
    "        random_mask = (mask_frac <= r) & (r < mask_frac + random_frac)\n",
    "\n",
    "        xs[pred_mask & mask_mask] = tokenizer.mask_token_id\n",
    "\n",
    "        random_input_ids = t.randint(\n",
    "            low=0, high=len(tokenizer), size=xs.shape, dtype=t.long, device=device\n",
    "        )\n",
    "        xs[pred_mask & random_mask] = random_input_ids[pred_mask & random_mask]\n",
    "\n",
    "        return xs, pred_mask, ys\n",
    "\n",
    "    return fn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.seed(0)\n",
    "\n",
    "device = \"cuda\"\n",
    "collate_fn = get_wiki_collate_fn(\n",
    "    max_seq_length=10,\n",
    "    tokenizer=transformers.AutoTokenizer.from_pretrained(\"bert-base-cased\"),\n",
    "    pred_frac=0.15,\n",
    "    mask_frac=1,\n",
    "    random_frac=0,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "dl_train_small = DataLoader(\n",
    "    random.sample(data_train, k=4),\n",
    "    batch_size=4,\n",
    "    collate_fn=collate_fn,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "dl_train = DataLoader(\n",
    "    data_train,\n",
    "    batch_size=16,\n",
    "    collate_fn=collate_fn,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "dl_test_small = DataLoader(\n",
    "    random.sample(data_test, k=256),\n",
    "    batch_size=16,\n",
    "    collate_fn=collate_fn,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "dl_test = DataLoader(\n",
    "    data_test,\n",
    "    batch_size=16,\n",
    "    collate_fn=collate_fn,\n",
    "    shuffle=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mlm_accuracy(model: nn.Module, dl: DataLoader) -> float:\n",
    "    num_correct: int = 0\n",
    "    num_total: int = 0\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    pbar = tqdm(dl, disable=True)\n",
    "    for x, pred_mask, y in pbar:\n",
    "        if len(y) == 0:\n",
    "            continue\n",
    "        logits = model(x)\n",
    "        pred_logits_flat = t.masked_select(logits, pred_mask.unsqueeze(-1))\n",
    "        pred_logits = pred_logits_flat.reshape((-1, logits.shape[-1]))\n",
    "        preds = t.argmax(pred_logits, dim=-1)\n",
    "\n",
    "        num_correct += (preds == y).sum()\n",
    "        num_total += len(y)\n",
    "        pbar.set_description(f'acc={num_correct / num_total:.2}')\n",
    "\n",
    "    if num_total == 0:\n",
    "        return 0.0\n",
    "    return num_correct / num_total\n",
    "\n",
    "def mlm_epoch(model: nn.Module, dl_train: DataLoader, dl_test: DataLoader, lr: float, opbar=None) -> nn.Module:\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)  # broken?\n",
    "    pbar = tqdm(enumerate(dl_train), leave=False)\n",
    "    for i, (x, pred_mask, y) in pbar:\n",
    "        if len(y) == 0:\n",
    "            continue\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(x)\n",
    "        pred_logits_flat = t.masked_select(logits, pred_mask.unsqueeze(-1))\n",
    "        pred_logits = pred_logits_flat.reshape((-1, logits.shape[-1]))\n",
    "        loss = F.cross_entropy(input=pred_logits, target=y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if i % 10 == 0:\n",
    "            pbar.set_description(f'loss={loss.item():.3f},acc={get_mlm_accuracy(model, dl_test):.3f}')\n",
    "            model.train()\n",
    "            if opbar is not None:\n",
    "                opbar.set_description(f'loss={loss.item():.3f},acc={get_mlm_accuracy(model, dl_test):.3f}')\n",
    "            # print(tokenizer.decode(y))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiny_bert = Bert(\n",
    "    vocab_size=28996,\n",
    "    hidden_size=256,\n",
    "    max_position_embeddings=512,\n",
    "    type_vocab_size=2,\n",
    "    dropout=0,#0.1,\n",
    "    intermediate_size=1024,\n",
    "    num_heads=12,\n",
    "    num_layers=2,\n",
    ")\n",
    "tiny_bert.to(device)\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss=3.330,acc=0.000: 100%|██████████| 1000/1000 [00:49<00:00, 20.28it/s]\n"
     ]
    }
   ],
   "source": [
    "tiny_bert.train()\n",
    "epochs = 1000\n",
    "pbar = tqdm(range(epochs))\n",
    "for i in pbar:\n",
    "    # print(i)\n",
    "    mlm_epoch(tiny_bert, dl_train_small, dl_train_small, 1e-4, pbar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feed_bert(tiny_bert, \"The fish loves to eat [MASK].\", tokenizer, top_k=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_bert(model: nn.Module, text: str, tokenizer, top_k: int = 10):\n",
    "    input_ids: List[int] = tokenizer(text)[\"input_ids\"]\n",
    "    mask_idxs = [idx for idx, token in enumerate(input_ids) if token == tokenizer.mask_token_id]\n",
    "    print(mask_idxs)\n",
    "\n",
    "    all_logits = model(t.tensor([input_ids], dtype=t.long))[0]\n",
    "\n",
    "    print(text)\n",
    "    for mask_idx in mask_idxs:\n",
    "        logits = all_logits[mask_idx]\n",
    "        probs = t.softmax(logits, dim=0)\n",
    "\n",
    "        top_logit_idxs = t.argsort(logits, descending=True)[:top_k]\n",
    "        top_logit_words = tokenizer.decode(top_logit_idxs)\n",
    "\n",
    "        t.set_printoptions(precision=9)\n",
    "        print(top_logit_words)\n",
    "        print(probs[top_logit_idxs])\n",
    "        print()\n",
    "        t.set_printoptions(precision=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS] A kits [MASK] [MASK] take [MASK] human form [SEP]', '[CLS] [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]', '[CLS] They [MASK] in the final of the 2012 [SEP]', '[CLS] [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]']\n",
      "##une may on met\n",
      "*****************************\n",
      "tensor([[[-11.6676, -11.5691, -11.5570, -11.4961, -11.3019, -11.2182, -11.0608,\n",
      "          -10.8353, -10.2363,  -9.4051],\n",
      "         [-11.6676, -11.5691, -11.5570, -11.4961, -11.3019, -11.2182, -11.0608,\n",
      "          -10.8353, -10.2363,  -9.4051],\n",
      "         [-11.6676, -11.5691, -11.5570, -11.4961, -11.3019, -11.2182, -11.0608,\n",
      "          -10.8353, -10.2363,  -9.4051]],\n",
      "\n",
      "        [[-11.6676, -11.5691, -11.5570, -11.4961, -11.3019, -11.2182, -11.0608,\n",
      "          -10.8353, -10.2363,  -9.4051],\n",
      "         [-11.6676, -11.5691, -11.5570, -11.4961, -11.3019, -11.2182, -11.0608,\n",
      "          -10.8353, -10.2363,  -9.4051],\n",
      "         [-11.6676, -11.5691, -11.5570, -11.4961, -11.3019, -11.2182, -11.0608,\n",
      "          -10.8353, -10.2363,  -9.4051]],\n",
      "\n",
      "        [[-11.6676, -11.5691, -11.5570, -11.4961, -11.3019, -11.2182, -11.0608,\n",
      "          -10.8353, -10.2363,  -9.4051],\n",
      "         [-11.6676, -11.5691, -11.5570, -11.4961, -11.3019, -11.2182, -11.0608,\n",
      "          -10.8353, -10.2363,  -9.4051],\n",
      "         [-11.6676, -11.5691, -11.5570, -11.4961, -11.3019, -11.2182, -11.0608,\n",
      "          -10.8353, -10.2363,  -9.4051]],\n",
      "\n",
      "        [[-11.6676, -11.5691, -11.5570, -11.4961, -11.3019, -11.2182, -11.0608,\n",
      "          -10.8353, -10.2363,  -9.4051],\n",
      "         [-11.6676, -11.5691, -11.5570, -11.4961, -11.3019, -11.2182, -11.0608,\n",
      "          -10.8353, -10.2363,  -9.4051],\n",
      "         [-11.6676, -11.5691, -11.5570, -11.4961, -11.3019, -11.2182, -11.0608,\n",
      "          -10.8353, -10.2363,  -9.4051]]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "-----------------------\n",
      "[4, 5, 7]\n",
      "[CLS] A kits [MASK] [MASK] take [MASK] human form [SEP]\n",
      "the of in final take\n",
      "tensor([0.290129870, 0.126359016, 0.069412179, 0.055401463, 0.047330841],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "\n",
      "the of in final take\n",
      "tensor([0.290129870, 0.126359016, 0.069412179, 0.055401463, 0.047330841],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "\n",
      "the of in final take\n",
      "tensor([0.290129870, 0.126359016, 0.069412179, 0.055401463, 0.047330841],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "\n",
      "---------------------------------------\n",
      "[]\n",
      "[CLS] [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "---------------------------------------\n",
      "[3]\n",
      "[CLS] They [MASK] in the final of the 2012 [SEP]\n",
      "the of in final take\n",
      "tensor([0.290129870, 0.126359016, 0.069412179, 0.055401463, 0.047330841],\n",
      "       device='cuda:0', grad_fn=<IndexBackward0>)\n",
      "\n",
      "---------------------------------------\n",
      "[]\n",
      "[CLS] [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "tiny_bert.eval()\n",
    "for (x, mask, y) in dl_train_small:\n",
    "    print(tokenizer.batch_decode(x))\n",
    "    print(tokenizer.decode(y))\n",
    "    print(\"*****************************\")\n",
    "    logits = tiny_bert(x)\n",
    "    #print(logits.shape)\n",
    "    print(t.sort(logits, dim=-1).values[:, :3, -10:])\n",
    "    print(\"-----------------------\")\n",
    "    for xx in x:\n",
    "        feed_bert(tiny_bert, tokenizer.decode(xx), tokenizer, top_k=5)\n",
    "        print(\"---------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_bert, _ = load_pretrained_bert(num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS] [MASK] kits [MASK] may take on [MASK] form [SEP]', '[CLS] [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]', '[CLS] [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]', '[CLS] They met in the [MASK] of the 2012 [SEP]']\n",
      "Aune human final\n",
      "*****************************\n",
      "-----------------------\n",
      "[2, 4, 8]\n",
      "[CLS] [MASK] kits [MASK] may take on [MASK] form [SEP]\n",
      "The These All For From\n",
      "tensor([0.243836567, 0.067158222, 0.028144445, 0.018736543, 0.018527528],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "\n",
      "also that which and,\n",
      "tensor([0.172607183, 0.137166083, 0.128834605, 0.048280545, 0.035198011],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "\n",
      "any this the kit a\n",
      "tensor([0.408585191, 0.173036709, 0.076863118, 0.069941096, 0.029075900],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "\n",
      "---------------------------------------\n",
      "[]\n",
      "[CLS] [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "---------------------------------------\n",
      "[]\n",
      "[CLS] [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "---------------------------------------\n",
      "[6]\n",
      "[CLS] They met in the [MASK] of the 2012 [SEP]\n",
      "middle beginning finals course end\n",
      "tensor([0.218595713, 0.178650647, 0.051509611, 0.047538940, 0.044952597],\n",
      "       grad_fn=<IndexBackward0>)\n",
      "\n",
      "---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "test_bert.eval()\n",
    "for (x, mask, y) in dl_train_small:\n",
    "    print(tokenizer.batch_decode(x))\n",
    "    print(tokenizer.decode(y))\n",
    "    print(\"*****************************\")\n",
    "    logits, _ = test_bert(x)\n",
    "    #print(logits.shape)\n",
    "    # print(t.sort(logits, dim=-1).values[:, :3, -10:])\n",
    "    print(\"-----------------------\")\n",
    "    for xx in x:\n",
    "        feed_bert(lambda x : test_bert(x)[0], tokenizer.decode(xx), tokenizer, top_k=5)\n",
    "        print(\"---------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.0390,  0.1328, -0.0412,  ...,  0.1419,  0.0393,  0.0362],\n",
       "        [-0.0796,  0.1322, -0.1050,  ...,  0.1515,  0.0561,  0.0403],\n",
       "        [-0.0453,  0.0650, -0.0387,  ...,  0.0973,  0.1245,  0.0768],\n",
       "        ...,\n",
       "        [-0.1224,  0.1273, -0.0526,  ...,  0.1105,  0.0831,  0.0459],\n",
       "        [-0.0134,  0.0756, -0.1486,  ...,  0.1267,  0.0428,  0.1165],\n",
       "        [-0.0816,  0.1368, -0.0750,  ...,  0.0707,  0.1486,  0.1435]],\n",
       "       device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tiny_bert.lm_head.unembedding.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-0.0315, -0.0883, -0.0943,  ..., -0.0812, -0.0664, -0.1027],\n",
       "       device='cuda:0', requires_grad=True)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tiny_bert.lm_head.unembedding.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
