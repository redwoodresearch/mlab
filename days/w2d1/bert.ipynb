{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "from torch import einsum\n",
    "from einops import rearrange, reduce, repeat\n",
    "import bert_tests\n",
    "import math\n",
    "import torch\n",
    "import einops\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention pattern raw MATCH!!!!!!!!\n",
      " SHAPE (2, 12, 3, 3) MEAN: -0.002625 STD: 0.1206 VALS [-0.02817 -0.1156 0.05283 -0.0527 0.06188 0.2343 -0.07567 0.1243 0.07904 0.09438...]\n"
     ]
    }
   ],
   "source": [
    "def raw_attention_pattern(token_activations, num_heads, project_query, project_key):\n",
    "    # shape: batch_size, seq_length, 768 -> batch_size, 12, seq_length, 64\n",
    "    keys = rearrange(project_key(token_activations), 'b seq_length (num_heads head_size) -> b num_heads seq_length head_size', num_heads=num_heads)\n",
    "    queries = rearrange(project_query(token_activations), 'b seq_length (num_heads head_size) -> b num_heads seq_length head_size', num_heads=num_heads)\n",
    "    # Matrix multiplication Q's seq_length * head_size matmul with K.T's head_size * seq_length\n",
    "    # AB[ij] = \\sum_k A[ik]B[kj]\n",
    "    head_size = keys.shape[-1]\n",
    "    return einsum('b l k h, b l q h -> b l k q', keys, queries)/math.sqrt(head_size)\n",
    "\n",
    "bert_tests.test_attention_pattern_fn(raw_attention_pattern)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention MATCH!!!!!!!!\n",
      " SHAPE (2, 3, 768) MEAN: 0.002312 STD: 0.1127 VALS [0.01782 -0.02442 0.04425 -0.1779 -0.04225 0.08087 0.008635 0.05621 0.1111 0.05355...]\n"
     ]
    }
   ],
   "source": [
    "def bert_attention(token_activations, \n",
    "                    num_heads: int, \n",
    "                    attention_pattern, \n",
    "                    project_value, \n",
    "                    project_output):\n",
    "    # Shape: batch_size * num_heads * k seq_length * q seq_length\n",
    "    # Softmax over k\n",
    "    softmaxed_attention_pattern = t.nn.Softmax(dim=2)(attention_pattern)\n",
    "        \n",
    "    # Shape of values: batch_size b, num_heads l, seq_length k/q?, head_size h\n",
    "    values = rearrange(project_value(token_activations), 'b seq_length (num_heads head_size) -> b num_heads seq_length head_size', num_heads=num_heads)\n",
    "    \n",
    "    # Shape of softmax attention_pattern: batch_size b, num_heads l, key seq_length k, query seq_length q\n",
    "    # Check this?? Because we did a softmax over dim k, we're doing a matrix multiplication over dim k\n",
    "    raw_output = einsum('b l k q, b l k h -> b q l h', softmaxed_attention_pattern, values)\n",
    "    # We want output shape: batch_size, seq_length, hidden_size (i.e. num_heads * head_size)\n",
    "    raw_output = rearrange(raw_output, 'b q l h -> b q (l h)')\n",
    "    return project_output(raw_output)\n",
    "    \n",
    "bert_tests.test_attention_fn(bert_attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert MATCH!!!!!!!!\n",
      " SHAPE (2, 3, 768) MEAN: -0.001554 STD: 0.1736 VALS [-0.08316 -0.09165 -0.03188 -0.03013 0.1001 0.09549 -0.1046 0.07742 0.0424 0.05553...]\n"
     ]
    }
   ],
   "source": [
    "class MultiHeadedSelfAttention(torch.nn.Module):\n",
    "    def __init__(self, num_heads: int, hidden_size:int):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.hidden_size = hidden_size\n",
    "        # By default, Linear layers have bias=True\n",
    "        self.project_query = torch.nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        self.project_key = torch.nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        self.project_value = torch.nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        self.project_output = torch.nn.Linear(self.hidden_size, self.hidden_size)\n",
    "        \n",
    "    def forward(self, input: torch.Tensor):\n",
    "        attention_pattern = raw_attention_pattern(input, self.num_heads, self.project_query, self.project_key)\n",
    "        return bert_attention(input, self.num_heads, attention_pattern, self.project_value, self.project_output)\n",
    "bert_tests.test_bert_attention(MultiHeadedSelfAttention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert mlp MATCH!!!!!!!!\n",
      " SHAPE (2, 3, 768) MEAN: -0.0001934 STD: 0.1044 VALS [-0.1153 0.1189 -0.0813 0.1021 0.0296 0.06182 0.0341 0.1446 0.2622 -0.08507...]\n"
     ]
    }
   ],
   "source": [
    "def bert_mlp(token_activations: torch.Tensor, linear_1: torch.nn.Module, linear_2: torch.nn.Module\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    BERT MLP block: Linear, GELU activation (following GPT), Linear \n",
    "    \"\"\"\n",
    "    out = linear_1(token_activations)\n",
    "    out = torch.nn.GELU()(out)\n",
    "    return linear_2(out)\n",
    "\n",
    "bert_tests.test_bert_mlp(bert_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertMLP(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    It should map a vector of length input_size to intermediate_size and then \n",
    "     back to input_size, with a bias on each Linear layer and a Gelu between them\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size: int, intermediate_size: int):\n",
    "        super().__init__()\n",
    "        self.linear_1 = torch.nn.Linear(input_size, intermediate_size)\n",
    "        self.linear_2 = torch.nn.Linear(intermediate_size, input_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return bert_mlp(x, self.linear_1, self.linear_2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer norm MATCH!!!!!!!!\n",
      " SHAPE (20, 10) MEAN: -9.537e-09 STD: 1.003 VALS [0.6906 -0.84 1.881 1.711 -0.5116 -0.9577 -0.1387 -0.6943 -0.6741 -0.4662...]\n"
     ]
    }
   ],
   "source": [
    "class LayerNorm(torch.nn.Module):\n",
    "    def __init__(self, normalized_dim: int, eps = 1e-05):\n",
    "        \"\"\"\n",
    "        Create parameters weight and bias, both of shape [normalized_dim], \n",
    "         initialized as ones and zeros respectively\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.normalized_dim = normalized_dim\n",
    "        self.weight = torch.nn.Parameter(torch.ones((normalized_dim,)))\n",
    "        self.bias = torch.nn.Parameter(torch.zeros((normalized_dim,)))\n",
    "        self.eps = eps\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        5 things we need: input, mean, var, weight, bias\n",
    "        \"\"\"\n",
    "        mean_x = t.mean(x, dim=-1, keepdim=True).detach()\n",
    "        std_x = t.std(x, dim=-1, unbiased=False, keepdim=True).detach()\n",
    "        x_out = (x-mean_x)/t.sqrt(std_x**2 + self.eps)\n",
    "        return x_out*self.weight + self.bias                         \n",
    "\n",
    "    # input.shape = [batch_size, seq_length, hidden_size] --> normalize over input[i,j,:] has mean 0, var 1 \n",
    "    #  if normalized_dim == hidden_size\n",
    "bert_tests.test_layer_norm(LayerNorm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert MATCH!!!!!!!!\n",
      " SHAPE (2, 3, 768) MEAN: -5.795e-09 STD: 1 VALS [0.007132 -0.04372 0.6502 -0.5972 -1.097 0.7267 0.1275 -0.6035 -0.2226 0.2145...]\n"
     ]
    }
   ],
   "source": [
    "class BertBlock(t.nn.Module):\n",
    "    def __init__(self, hidden_size: int, intermediate_size: int, num_heads: int, dropout: float):\n",
    "        super().__init__()\n",
    "        # TODO: Clean up\n",
    "        self.layer1 = MultiHeadedSelfAttention(num_heads, hidden_size)\n",
    "        self.layer_norm1 = LayerNorm(hidden_size)\n",
    "        self.layer2 = t.nn.Sequential(\n",
    "            BertMLP(hidden_size, intermediate_size),\n",
    "            t.nn.Dropout(dropout),\n",
    "        )\n",
    "        self.layer_norm2 = LayerNorm(hidden_size)\n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x) + x\n",
    "        out = self.layer_norm1(out)\n",
    "        out = self.layer2(out) + out\n",
    "        return self.layer_norm2(out)\n",
    "        \n",
    "bert_tests.test_bert_block(BertBlock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding MATCH!!!!!!!!\n",
      " SHAPE (2, 3, 5) MEAN: -0.06748 STD: 1.062 VALS [1.176 -0.1914 0.8212 1.047 -0.481 0.7106 -1.304 -1.307 -0.438 -0.2764...]\n"
     ]
    }
   ],
   "source": [
    "class Embedding(torch.nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size):\n",
    "        super().__init__()\n",
    "        self.token_embedding_matrix = torch.nn.Parameter(torch.randn(vocab_size, embed_size))\n",
    "        \n",
    "    def forward(self, input: torch.LongTensor) -> torch.FloatTensor:\n",
    "        token_embeddings = self.token_embedding_matrix[input]\n",
    "        return token_embeddings\n",
    "        \n",
    "bert_tests.test_embedding(Embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[1.4000, 1.5000],\n",
       "         [2.4000, 2.5000]],\n",
       "\n",
       "        [[2.4000, 2.5000],\n",
       "         [2.4000, 2.5000]]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = t.tensor([[0, 1], [1, 1]], dtype=torch.long)\n",
    "B = t.Tensor([[1.4, 1.5], [2.4, 2.5]])\n",
    "B[A]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert embedding MATCH!!!!!!!!\n",
      " SHAPE (2, 3, 768) MEAN: 4.967e-09 STD: 1 VALS [-1.319 -0.4378 -2.074 0.9679 0.9274 1.479 -0.501 -1.9 -0.212 0.7961...]\n"
     ]
    }
   ],
   "source": [
    "def bert_embedding(\n",
    "        input_ids, \n",
    "        token_type_ids, \n",
    "        position_embedding: Embedding,\n",
    "        token_embedding: Embedding, \n",
    "        token_type_embedding: Embedding, \n",
    "        layer_norm: LayerNorm, \n",
    "        dropout: torch.nn.Dropout):\n",
    "    # Get device tensor is stored on\n",
    "    device = input_ids.get_device()\n",
    "    with torch.cuda.device(device):\n",
    "        position_inds = einops.repeat(torch.arange(input_ids.shape[-1]), 'l -> b l', b=input_ids.shape[0])\n",
    "        # input_ids shape: batch_size, seq_length\n",
    "        # For each of these, we're indexing the lookup table\n",
    "        out = position_embedding(position_inds)\n",
    "        out += token_embedding(input_ids)\n",
    "        out += token_type_embedding(token_type_ids)\n",
    "        out = layer_norm(out)\n",
    "        return dropout(out)\n",
    "    \n",
    "bert_tests.test_bert_embedding_fn(bert_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert embedding MATCH!!!!!!!!\n",
      " SHAPE (2, 3, 768) MEAN: -3.725e-09 STD: 1 VALS [-0.009385 -0.4919 0.9852 -0.3535 -3.624 1.333 1.163 1.449 1.063 0.246...]\n"
     ]
    }
   ],
   "source": [
    "class BertEmbedding(t.nn.Module):\n",
    "    def __init__(self, vocab_size: int, hidden_size: int, max_position_embeddings: int, type_vocab_size: int, dropout: float):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.token_embedding = Embedding(vocab_size, hidden_size)\n",
    "        self.position_embedding = Embedding(max_position_embeddings, hidden_size)\n",
    "        self.token_type_embedding = Embedding(type_vocab_size, hidden_size)\n",
    "        self.layer_norm = LayerNorm(hidden_size)\n",
    "        self.dropout = t.nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, input_ids, token_type_ids):\n",
    "        return bert_embedding(input_ids, token_type_ids, self.position_embedding, \n",
    "                              self.token_embedding, self.token_type_embedding, self.layer_norm, self.dropout)\n",
    "    \n",
    "bert_tests.test_bert_embedding(BertEmbedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert MATCH!!!!!!!!\n",
      " SHAPE (1, 4, 28996) MEAN: 0.003031 STD: 0.5765 VALS [-0.5742 -0.432 0.1186 -0.7165 -0.5261 0.4967 1.223 0.3165 -0.3247 -0.5716...]\n"
     ]
    }
   ],
   "source": [
    "class Bert(t.nn.Module):\n",
    "    def __init__(\n",
    "            self, vocab_size: int, hidden_size: int, \n",
    "            max_position_embeddings: int, type_vocab_size: int, \n",
    "            dropout: float, intermediate_size: int, num_heads: int, \n",
    "            num_layers: int\n",
    "        ):\n",
    "        super().__init__()\n",
    "        self.embedding = BertEmbedding(vocab_size, hidden_size, max_position_embeddings, type_vocab_size, dropout)\n",
    "        self.bertblocks = t.nn.Sequential(\n",
    "            *[BertBlock(hidden_size, intermediate_size, num_heads, dropout) for _ in range(num_layers)]\n",
    "        )\n",
    "       \n",
    "        self.lin = t.nn.Linear(hidden_size, hidden_size)\n",
    "        self.norm = LayerNorm(hidden_size)\n",
    "        self.unembed = t.nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "    def forward(self, input_ids):\n",
    "        device = input_ids.get_device()\n",
    "        with torch.cuda.device(device):\n",
    "            token_type_ids = torch.zeros(input_ids.shape, dtype=int)\n",
    "            out = self.embedding(input_ids, token_type_ids)\n",
    "            out = self.bertblocks(out)\n",
    "            # Last layers to map to output\n",
    "            out = self.lin(out)\n",
    "            out = t.nn.functional.gelu(out)\n",
    "            out = self.norm(out)\n",
    "            out = self.unembed(out)\n",
    "            return out\n",
    "        \n",
    "bert_tests.test_bert(Bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "my_bert = Bert(\n",
    "    vocab_size=28996, hidden_size=768, max_position_embeddings=512, \n",
    "    type_vocab_size=2, dropout=.1, intermediate_size=3072, \n",
    "    num_heads=12, num_layers=12\n",
    ")\n",
    "pretrained_bert = bert_tests.get_pretrained_bert()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretrained_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding.token_embedding.weight                        embedding.token_embedding.token_embedding_matrix\n",
      "embedding.position_embedding.weight                     embedding.position_embedding.token_embedding_matrix\n",
      "embedding.token_type_embedding.weight                   embedding.token_type_embedding.token_embedding_matrix\n",
      "embedding.layer_norm.weight                             embedding.layer_norm.weight\n",
      "embedding.layer_norm.bias                               embedding.layer_norm.bias\n",
      "transformer.0.layer_norm.weight                         bertblocks.0.layer1.project_query.weight\n",
      "transformer.0.layer_norm.bias                           bertblocks.0.layer1.project_query.bias\n",
      "transformer.0.attention.pattern.project_query.weight    bertblocks.0.layer1.project_key.weight\n",
      "transformer.0.attention.pattern.project_query.bias      bertblocks.0.layer1.project_key.bias\n",
      "transformer.0.attention.pattern.project_key.weight      bertblocks.0.layer1.project_value.weight\n",
      "transformer.0.attention.pattern.project_key.bias        bertblocks.0.layer1.project_value.bias\n",
      "transformer.0.attention.project_value.weight            bertblocks.0.layer1.project_output.weight\n",
      "transformer.0.attention.project_value.bias              bertblocks.0.layer1.project_output.bias\n",
      "transformer.0.attention.project_out.weight              bertblocks.0.layer_norm1.weight\n",
      "transformer.0.attention.project_out.bias                bertblocks.0.layer_norm1.bias\n",
      "transformer.0.residual.mlp1.weight                      bertblocks.0.layer2.0.linear_1.weight\n",
      "transformer.0.residual.mlp1.bias                        bertblocks.0.layer2.0.linear_1.bias\n",
      "transformer.0.residual.mlp2.weight                      bertblocks.0.layer2.0.linear_2.weight\n",
      "transformer.0.residual.mlp2.bias                        bertblocks.0.layer2.0.linear_2.bias\n",
      "transformer.0.residual.layer_norm.weight                bertblocks.0.layer_norm2.weight\n",
      "transformer.0.residual.layer_norm.bias                  bertblocks.0.layer_norm2.bias\n",
      "lm_head.mlp.weight                                      lin.weight\n",
      "lm_head.mlp.bias                                        lin.bias\n",
      "lm_head.unembedding.weight                              norm.weight\n",
      "lm_head.unembedding.bias                                norm.bias\n",
      "lm_head.layer_norm.weight                               unembed.weight\n",
      "lm_head.layer_norm.bias                                 unembed.bias\n"
     ]
    }
   ],
   "source": [
    "for param1, param2 in zip(pretrained_bert.state_dict(), my_bert.state_dict()):\n",
    "    if param1.startswith('transformer') and not(param1.startswith('transformer.0')):\n",
    "        continue\n",
    "    print(param1.ljust(55), param2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = \"\"\"embedding.token_embedding.weight embedding.token_embedding.token_embedding_matrix\n",
    "embedding.position_embedding.weight embedding.position_embedding.token_embedding_matrix\n",
    "embedding.token_type_embedding.weight embedding.token_type_embedding.token_embedding_matrix\n",
    "embedding.layer_norm.weight embedding.layer_norm.weight\n",
    "embedding.layer_norm.bias embedding.layer_norm.bias\n",
    "lm_head.mlp.weight lin.weight\n",
    "lm_head.mlp.bias lin.bias\n",
    "lm_head.unembedding.weight unembed.weight \n",
    "lm_head.unembedding.bias unembed.bias\n",
    "lm_head.layer_norm.weight norm.weight\n",
    "lm_head.layer_norm.bias norm.bias\n",
    "transformer.(\\d+).layer_norm.weight bertblocks.\\g<1>.layer_norm1.weight\n",
    "transformer.(\\d+).layer_norm.bias bertblocks.\\g<1>.layer_norm1.bias\n",
    "transformer.(\\d+).attention.pattern.project_query.weight bertblocks.\\g<1>.layer1.project_query.weight\n",
    "transformer.(\\d+).attention.pattern.project_query.bias bertblocks.\\g<1>.layer1.project_query.bias\n",
    "transformer.(\\d+).attention.pattern.project_key.weight bertblocks.\\g<1>.layer1.project_key.weight\n",
    "transformer.(\\d+).attention.pattern.project_key.bias bertblocks.\\g<1>.layer1.project_key.bias\n",
    "transformer.(\\d+).attention.project_value.weight bertblocks.\\g<1>.layer1.project_value.weight\n",
    "transformer.(\\d+).attention.project_value.bias bertblocks.\\g<1>.layer1.project_value.bias\n",
    "transformer.(\\d+).attention.project_out.weight bertblocks.\\g<1>.layer1.project_output.weight\n",
    "transformer.(\\d+).attention.project_out.bias bertblocks.\\g<1>.layer1.project_output.bias\n",
    "transformer.(\\d+).residual.mlp1.weight bertblocks.\\g<1>.layer2.0.linear_1.weight\n",
    "transformer.(\\d+).residual.mlp1.bias bertblocks.\\g<1>.layer2.0.linear_1.bias\n",
    "transformer.(\\d+).residual.mlp2.weight bertblocks.\\g<1>.layer2.0.linear_2.weight\n",
    "transformer.(\\d+).residual.mlp2.bias bertblocks.\\g<1>.layer2.0.linear_2.bias\n",
    "transformer.(\\d+).residual.layer_norm.weight bertblocks.\\g<1>.layer_norm2.weight\n",
    "transformer.(\\d+).residual.layer_norm.bias bertblocks.\\g<1>.layer_norm2.bias\"\"\"\n",
    "\n",
    "\n",
    "def mapkey(key):\n",
    "    for line in params.split('\\n'):\n",
    "        param1, param2 = line.split(' ')[:2]\n",
    "        # Check for match\n",
    "        if re.match(param1, key):\n",
    "            return re.sub(param1, param2, key)\n",
    "    print(f\"{key} does not have corresponding name\")\n",
    "        \n",
    "new_state_dict  = OrderedDict({mapkey(k): v for k, v in pretrained_bert.state_dict().items() if 'classification_head' not in k})\n",
    "my_bert.load_state_dict(new_state_dict, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comparing Berts MATCH!!!!!!!!\n",
      " SHAPE (10, 20, 28996) MEAN: -2.732 STD: 2.413 VALS [-5.65 -6.041 -6.096 -6.062 -5.946 -5.777 -5.977 -6.015 -6.028 -5.935...]\n"
     ]
    }
   ],
   "source": [
    "bert_tests.test_same_output(my_bert, pretrained_bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get state_dict from pretrained BERT\n",
    "\n",
    "# Create new dictionary with the new names for our BERT (this uses mapkey)\n",
    "\n",
    "# my_bert.load_state_dict(new_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class MLP1(torch.nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         self.linear1 = torch.nn.Linear(5, 5)\n",
    "#     def forward(self, x):\n",
    "#         return self.linear1(x)\n",
    "# class MLP2(torch.nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         self.linear2 = torch.nn.Linear(5, 5)\n",
    "#     def forward(self, x):\n",
    "#         return self.linear2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlp1 = MLP1()\n",
    "# mlp2 = MLP2()\n",
    "# x = torch.randn((5,5))\n",
    "# mlp1(x)\n",
    "# mlp2(x)\n",
    "# print(mlp1.state_dict())\n",
    "# print(mlp2.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mlp1.load_state_dict(mlp2.state_dict(), strict=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import random\n",
    "# W2D2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Works with TOKENIZERS_PARALLELISM=false python w2d2.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 7632, 1010, 2026, 2171, 2003, 14324, 102]\n",
      "Uncased: [CLS] hi, my name is bert [SEP]\n",
      "Cased: [CLS] colleges å¤© largest happened smile donation [SEP]\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "tokenizer_uncased = transformers.AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "text = \"Hi, my name is bert\"\n",
    "\n",
    "tokenized_text = tokenizer_uncased.encode(text)\n",
    "print(tokenized_text)\n",
    "\n",
    "import os\n",
    "# So Jupyter doesn't crash\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"]='false'\n",
    "\n",
    "# tokenizer.decode(): https://huggingface.co/docs/transformers/main_classes/tokenizer\n",
    "print(f\"Uncased: {tokenizer_uncased.decode(tokenized_text, errors='ignore')}\")\n",
    "\n",
    "# tokenizer.decode(): https://huggingface.co/docs/transformers/main_classes/tokenizer\n",
    "print(f\"Cased: {tokenizer.decode(tokenized_text, errors='ignore')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103\n",
      "MASK at index 8: [('red', 0.5602869391441345), ('yellow', 0.12122765183448792), ('white', 0.07108739763498306), ('blue', 0.06910934299230576), ('green', 0.05439632758498192)]\n"
     ]
    }
   ],
   "source": [
    "def ascii_art_probs(s):\n",
    "    masked_token = tokenizer.encode('[MASK]')[1]\n",
    "    print(masked_token)\n",
    "    tokenized_text = tokenizer.encode(s)\n",
    "    masked_inds = [i for i, token in enumerate(tokenized_text) if token==masked_token]\n",
    "    tokenized_text = t.tensor(tokenized_text, dtype=t.long)\n",
    "    # Shape: batch_size, seq_length, hidden_size\n",
    "    # print(tokenized_text)\n",
    "    # We need an unsqueeze because we took the first dimension as the batch in our bert_embedding code:\n",
    "    #  position_inds = einops.repeat(torch.arange(input_ids.shape[-1]), 'l -> b l', b=input_ids.shape[0])\n",
    "\n",
    "    tokenized_text = tokenized_text.unsqueeze(0)\n",
    "    x = my_bert(tokenized_text)\n",
    "    # Normalizing over the class\n",
    "    probabilities = t.nn.Softmax(dim=2)(x)\n",
    "    topk = t.topk(probabilities, k=5, dim=2).indices[0]\n",
    "    probsk = t.topk(probabilities, k=5, dim=2).values[0].detach()\n",
    "    # Iterating over the tokens\n",
    "    for i, (ids, probs) in enumerate(zip(topk, probsk)):\n",
    "        answer = [tokenizer.decode(i) for i in ids]\n",
    "        # Sanity check\n",
    "        # print(*[(a, float(p)) for a, p in zip(answer, list(probs))])\n",
    "        if i in masked_inds:\n",
    "            print(f\"MASK at index {i}: {[(a, float(p)) for a, p in zip(answer, list(probs))]}\")\n",
    "        \n",
    "ascii_art_probs(\"The firetruck was painted bright [MASK].\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103\n",
      ". . fire ##tr ##uck was painted bright red . . \n",
      ", \" fireplace ##dr ##ucks is stained brightly yellow ; ; \n",
      "the the Fire ##cre ##out wasn a dark white ! ! \n",
      ") , fires ##fo ##ub were burned yellow blue ? ? \n",
      "\" ; flames ##be ##wing been still black green : \" \n"
     ]
    }
   ],
   "source": [
    "def ascii_art_probs2(s):\n",
    "    masked_token = tokenizer.encode('[MASK]')[1]\n",
    "    print(masked_token)\n",
    "    tokenized_text = tokenizer.encode(s)\n",
    "    masked_inds = [i for i, token in enumerate(tokenized_text) if token==masked_token]\n",
    "    tokenized_text = t.tensor(tokenized_text, dtype=t.long)\n",
    "    # Shape: batch_size, seq_length, hidden_size\n",
    "    # print(tokenized_text)\n",
    "    # We need an unsqueeze because we took the first dimension as the batch in our bert_embedding code:\n",
    "    #  position_inds = einops.repeat(torch.arange(input_ids.shape[-1]), 'l -> b l', b=input_ids.shape[0])\n",
    "\n",
    "    tokenized_text = tokenized_text.unsqueeze(0)\n",
    "    x = my_bert(tokenized_text)\n",
    "    # Normalizing over the class\n",
    "    probabilities = t.nn.Softmax(dim=2)(x)\n",
    "    topk = t.topk(probabilities, k=5, dim=2).indices[0]\n",
    "    probsk = t.topk(probabilities, k=5, dim=2).values[0].detach()\n",
    "    # Iterating over the tokens\n",
    "    for rank in range(5):\n",
    "        for i, (ids, probs) in enumerate(zip(topk, probsk)):\n",
    "            answer = [tokenizer.decode(i) for i in ids]\n",
    "            # Sanity check\n",
    "            # print(*[(a, float(p)) for a, p in zip(answer, list(probs))])\n",
    "            # if i in masked_inds:\n",
    "            #     print(f\"MASK at index {i}: {[(a, float(p)) for a, p in zip(answer, list(probs))]}\")\n",
    "            print(answer[rank], end=' ')\n",
    "        print()\n",
    "        \n",
    "ascii_art_probs2(\"The firetruck was painted bright [MASK].\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretrained_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert MATCH!!!!!!!!\n",
      " SHAPE (1, 4, 28996) MEAN: 0.003031 STD: 0.5765 VALS [-0.5742 -0.432 0.1186 -0.7165 -0.5261 0.4967 1.223 0.3165 -0.3247 -0.5716...]\n",
      "bert MATCH!!!!!!!!\n",
      " SHAPE (1, 2) MEAN: 0.09479 STD: 1.411 VALS [-0.903 1.093]\n"
     ]
    }
   ],
   "source": [
    "class BertClassifier(t.nn.Module):\n",
    "    def __init__(\n",
    "            self, vocab_size: int, hidden_size: int, \n",
    "            max_position_embeddings: int, type_vocab_size: int, \n",
    "            dropout: float, intermediate_size: int, num_heads: int, \n",
    "            num_layers: int, num_classes: int\n",
    "        ):\n",
    "        super().__init__()\n",
    "        self.embedding = BertEmbedding(vocab_size, hidden_size, max_position_embeddings, type_vocab_size, dropout)\n",
    "        self.bertblocks = t.nn.Sequential(\n",
    "            *[BertBlock(hidden_size, intermediate_size, num_heads, dropout) for _ in range(num_layers)]\n",
    "        )\n",
    "       \n",
    "        # This maps embeddings to word logits -- word_logits_map\n",
    "        self.lin = t.nn.Linear(hidden_size, hidden_size)\n",
    "        self.norm = LayerNorm(hidden_size)\n",
    "        self.unembed = t.nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "        # This maps embeddings to classification logits -- classification_logits_map\n",
    "        self.dropout = torch.nn.Dropout(dropout)\n",
    "        self.classification_layer = torch.nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "        \n",
    "    def forward(self, input_ids):\n",
    "\n",
    "        device = input_ids.get_device()\n",
    "        with torch.cuda.device(device):\n",
    "            token_type_ids = torch.zeros(input_ids.shape, dtype=int)\n",
    "            out = self.embedding(input_ids, token_type_ids)\n",
    "            embeddings = self.bertblocks(out)\n",
    "            \n",
    "            # This maps embeddings to word logits -- word_logits_map\n",
    "            out = self.lin(embeddings)\n",
    "            out = t.nn.functional.gelu(out)\n",
    "            out = self.norm(out)\n",
    "            word_logits = self.unembed(out)\n",
    "            \n",
    "            # This maps embeddings to classification logits -- classification_logits_map\n",
    "            out = self.dropout(embeddings)\n",
    "            # out.shape = [batch, seq_length, embed_size]\n",
    "            # We're only interested in the first token ([CLS]) for classification\n",
    "            classification_logits = self.classification_layer(out)\n",
    "            return word_logits, classification_logits[:,0] \n",
    "        \n",
    "bert_tests.test_bert_classification(BertClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "data_train, data_test = torchtext.datasets.IMDB(root='.data', split=('train', 'test'))\n",
    "data_train = list(data_train)\n",
    "data_test = list(data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.encode(saved_data[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_list(data, max_seq_len):\n",
    "    \"\"\"Take in data_train or data_test, and output a dataloader (an iterable of sample x, label y)\"\"\"\n",
    "    # Reverse sample and label\n",
    "    # Tokenize\n",
    "    saved_data = list(data)\n",
    "    assert len(saved_data), \"No data left!\"\n",
    "    labels = [label for label, review in saved_data]\n",
    "    # reviews = [review for label, review in saved_data]\n",
    "    # tokenized_reviews = tokenizer.encode(reviews)\n",
    "    reviews = [review for label, review in saved_data]\n",
    "    tokenized_reviews = tokenizer(reviews)['input_ids']\n",
    "    \n",
    "    # Truncate based on max_seq_len\n",
    "    tokenized_reviews = [tokens[:max_seq_len] for tokens in tokenized_reviews]\n",
    "    \n",
    "    # Get padding token_id\n",
    "    # Pad to longest\n",
    "    longest_length = max([len(tokens) for tokens in tokenized_reviews])\n",
    "    tokenized_reviews = [tokens + [tokenizer.pad_token_id]*(longest_length-len(tokens)) for tokens in tokenized_reviews]\n",
    "    \n",
    "    # Shuffle\n",
    "    data_list = [(tokens, 0 if label == 'neg' else 1) for tokens, label in zip(tokenized_reviews, labels)]\n",
    "    random.shuffle(data_list)\n",
    "    return data_list\n",
    "    \n",
    "def get_batches(data, batch_size, max_seq_len):\n",
    "    shuffled_data_list = get_data_list(data, max_seq_len)\n",
    "    res = []\n",
    "    for batch_idx in range(math.ceil(len(shuffled_data_list) / batch_size)):\n",
    "        batch_data = shuffled_data_list[batch_idx*batch_size: (batch_idx+1)*batch_size]\n",
    "        reviews = [review for review, sentiment in batch_data]\n",
    "        sentiments = [sentiment for review, sentiment in batch_data]\n",
    "        res.append((torch.tensor(reviews, dtype=torch.long), torch.tensor(sentiments, dtype=torch.long)))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 Train your model!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_IncompatibleKeys(missing_keys=['classification_layer.weight', 'classification_layer.bias'], unexpected_keys=[])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load weights with strict=False\n",
    "classifier_bert = BertClassifier(\n",
    "    vocab_size=28996, hidden_size=768, max_position_embeddings=512, \n",
    "    type_vocab_size=2, dropout=.1, intermediate_size=3072, \n",
    "    num_heads=12, num_layers=12, num_classes=2\n",
    ")\n",
    "\n",
    "new_state_dict  = OrderedDict({mapkey(k): v for k, v in pretrained_bert.state_dict().items() if 'classification_head' not in k})\n",
    "classifier_bert.load_state_dict(new_state_dict, strict=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretrained_bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (521 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "max_seq_len = 256\n",
    "batch_size = 16\n",
    "train_dataloader = get_batches(data_train, batch_size, max_seq_len)\n",
    "lr = 1e-05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_steps = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# module.training\n",
    "# module.train() --> module.training = True\n",
    "# module.eval() --> module.training = False\n",
    "\n",
    "# def forward(...):\n",
    "#     if self.training:\n",
    "#         ....\n",
    "#     else:\n",
    "#         ....\n",
    "\n",
    "device = 'cuda'\n",
    "# with t.cuda.device(0):\n",
    "optimizer = t.optim.Adam(classifier_bert.parameters(), lr = lr)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "classifier_bert.train()\n",
    "classifier_bert.to(device)\n",
    "for X, Y in train_dataloader:\n",
    "    gradient_steps += 1\n",
    "    optimizer.zero_grad()\n",
    "    word_logits, classification_logits = classifier_bert(X.to(device))\n",
    "    # Get predicted classes from logits\n",
    "    # predictions = t.nn.SoftMax(dim = -1)(classification_logits)\n",
    "    loss = loss_fn(classification_logits, Y.to(device))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if gradient_steps % 100 == 0:\n",
    "        print(gradient_steps, loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_params = torch.nn.Linear(1,1)\n",
    "# tmp_params.to(device)\n",
    "with torch.cuda.device(0):\n",
    "    print(tmp_params.weight.get_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neg:0.97 This movie sucked. I hated it.\n",
      "neg:0.68 I fell asleep while watching this movie because it was super boring\n",
      "pos:0.97 My friends and I loved it.\n",
      "pos:0.71 I was super scared by the movie\n",
      "pos:0.99 This was my favorite movie ever!\n",
      "pos:0.68 This was good and bad\n",
      "neg:0.53 This was bad and good\n",
      "neg:0.76 I must applaud the creators of this movie. It's incredible that they've managed to write such an uninspired piece.\n"
     ]
    }
   ],
   "source": [
    "examples =  ['This movie sucked. I hated it.', \n",
    "             'I fell asleep while watching this movie because it was super boring',\n",
    "             'My friends and I loved it.',\n",
    "             'I was super scared by the movie',\n",
    "             'This was my favorite movie ever!',\n",
    "             'This was good and bad',\n",
    "             'This was bad and good',\n",
    "             'I must applaud the creators of this movie. It\\'s incredible that they\\'ve managed to write such an uninspired piece.']\n",
    "classifier_bert.eval()\n",
    "for example in examples:\n",
    "    # Encode\n",
    "    encoded_tokens = tokenizer.encode(example)\n",
    "    # Convert to tensor\n",
    "    input_ids = torch.tensor(encoded_tokens, dtype=torch.long).unsqueeze(0)\n",
    "    # Predict\n",
    "    word_logits, classification_logits = classifier_bert(input_ids)\n",
    "    # softmax\n",
    "    softmaxed_classification_logits = torch.nn.Softmax(dim=-1)(classification_logits)\n",
    "    # Argmax\n",
    "    prediction = torch.argmax(softmaxed_classification_logits, dim=-1)\n",
    "    # Print\n",
    "    neg, pos = softmaxed_classification_logits[0]\n",
    "    print(f'neg:{neg:.2}' if prediction.item()==0 else f'pos:{pos:.2}', example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part Three: Training from Scratch on Masked Language Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikitext2_dataset = torchtext.datasets.WikiText2(root='.data', split=('train', 'valid', 'test'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_wiki_data, val_wiki_data, test_wiki_data = wikitext2_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_wiki_data = list(train_wiki_data)\n",
    "val_wiki_data = list(val_wiki_data)\n",
    "test_wiki_data = list(test_wiki_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_wiki_data[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 1147, 2491, 100, 119, 102]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"their lives [UNK] . \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[101, 1147, 2491, 28996, 119, 102]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer_with_unk = transformers.AutoTokenizer.from_pretrained(\"bert-base-cased\", unk_token='<unk>')\n",
    "tokenizer_with_unk.encode(\"their lives <unk> . \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9672, 0.5151])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t.rand((2,))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sldkfj [UNK] sldfj[UNK]ldk'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'sldkfj <unk> sldfj<unk>ldk'.replace('<unk>', '[UNK]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(True)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(1).lt(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_seq(tokens, max_seq_len):\n",
    "    if isinstance(tokens, torch.Tensor):\n",
    "        tokens = [token.item() for token in tokens]\n",
    "    return tokens + [tokenizer.pad_token_id]*(max_seq_len-len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.3207430839538574, 0.8816239833831787]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x.item() for x in torch.rand((2,))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 2, 3), (4, 5, 6)]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = list(zip([1,2,3], [4,5,6]))\n",
    "list(zip(*tmp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try doing this on a sequence first\n",
    "# Then try modifying get_data_list to do this\n",
    "# Arguments\n",
    "sentence = 'hello this is a sentence <unk> here something something.'\n",
    "max_seq_len = 512  # TODO: Replace this\n",
    "# Find id of token[MASK]\n",
    "\n",
    "\n",
    "def get_wiki_data_list(sentences: list, max_seq_len: int, min_seq_len: int = 10):\n",
    "    masked_token = tokenizer.encode('[MASK]')[1] # [CLS] [MASK] [END]\n",
    "    masked_sentences = []\n",
    "    unmasked_sentences = []\n",
    "    masks = []\n",
    "    vocab_size = 28996\n",
    "    for sentence in sentences:\n",
    "        # Replace <unk> with [UNK]\n",
    "        sentence = sentence.replace('<unk>', '[UNK]')\n",
    "        # Tokenize\n",
    "        sentence = tokenizer.encode(sentence)\n",
    "        \n",
    "        # Skip those lower than minimum no. of tokens e.g. 10\n",
    "        # print(sentence)\n",
    "        if len(sentence) >= min_seq_len:\n",
    "            sentence = torch.tensor(sentence[:max_seq_len], dtype=torch.long)\n",
    "            initial_len = len(sentence)  # Sanity check\n",
    "            unmasked_sentences.append(pad_seq(sentence, max_seq_len).copy())\n",
    "            assert len(sentence) == initial_len  # Sanity check: new sentence was padded, not the original one\n",
    "            # Randomly replace tokens with [MASK] according to BERT paper\n",
    "            probs = torch.rand(len(sentence))\n",
    "            # mask 15% of all tokens at random \n",
    "            # Of these, replace with [MASK] 80% of the time, a random token 10%, and unchanged 10%\n",
    "            sentence[probs < 0.8*0.15] = masked_token    \n",
    "            num_random_tokens = len(sentence[(probs < .9*0.15) & (probs >= .8*.15)])\n",
    "            sentence[(probs < .9*0.15) & (probs >= 0.8*.15)] = torch.randint(vocab_size, (num_random_tokens,))\n",
    "            # Pad sentences\n",
    "            sentence = pad_seq(sentence, max_seq_len)\n",
    "            masked_sentences.append(sentence.copy())\n",
    "            mask = [x.item() for x in (probs < 0.15)] + [0]*(max_seq_len-initial_len)\n",
    "            masks.append(mask)\n",
    "            # print(tokenizer.decode(sentence))\n",
    "\n",
    "    # Shuffle for all the outputs\n",
    "    out = list(zip(masked_sentences, masks, unmasked_sentences))\n",
    "    random.shuffle(out)\n",
    "    \n",
    "    # Return arr of zero/one for mask, and an arr for actual IDs\n",
    "    return list(zip(*out))\n",
    "\n",
    "def get_wiki_batches(data, batch_size, max_seq_len):\n",
    "    masked_sentences, masks, unmasked_sentences = get_wiki_data_list(data, max_seq_len)\n",
    "    res = []\n",
    "    \n",
    "    for batch_idx in range(math.ceil(len(masks) / batch_size)):\n",
    "        i, j = batch_idx*batch_size, (batch_idx+1)*batch_size \n",
    "        # print(masked_sentences[i:j])\n",
    "        # print(masks[i:j])\n",
    "        # print(unmasked_sentences[i:j])\n",
    "        res.append([t.tensor(x[i:j], dtype=torch.long) for x in [masked_sentences, masks, unmasked_sentences]])\n",
    "    # Stack the tensors: I want dim 0 be of size batch_size\n",
    "    return res\n",
    "\n",
    "print (get_wiki_batches(train_wiki_data[:10], 3, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tiny_bert = Bert(\n",
    "    vocab_size=28996, hidden_size=256, max_position_embeddings=512, \n",
    "    type_vocab_size=1, dropout=.1, intermediate_size=1024, \n",
    "    num_heads=8, num_layers=2)\n",
    "gradient_steps = 0\n",
    "train_dataloader = get_wiki_batches(train_wiki_data, 16, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 100 loss: 0.16195\n",
      "epoch: 101 loss: 0.14493\n",
      "epoch: 102 loss: 0.16115\n",
      "epoch: 103 loss: 0.12537\n",
      "epoch: 104 loss: 0.1354\n",
      "epoch: 105 loss: 0.13421\n",
      "epoch: 106 loss: 0.1174\n",
      "epoch: 107 loss: 0.21031\n",
      "epoch: 108 loss: 0.14917\n",
      "epoch: 109 loss: 0.21695\n",
      "epoch: 110 loss: 0.18115\n",
      "epoch: 111 loss: 0.15084\n",
      "epoch: 112 loss: 0.15101\n",
      "epoch: 113 loss: 0.20933\n",
      "epoch: 114 loss: 0.55942\n",
      "epoch: 115 loss: 0.51484\n",
      "epoch: 116 loss: 0.52889\n",
      "epoch: 117 loss: 0.49307\n",
      "epoch: 118 loss: 0.47989\n",
      "epoch: 119 loss: 0.46775\n",
      "epoch: 120 loss: 0.46776\n",
      "epoch: 121 loss: 0.44455\n",
      "epoch: 122 loss: 0.47442\n",
      "epoch: 123 loss: 0.4953\n",
      "epoch: 124 loss: 0.47437\n",
      "epoch: 125 loss: 0.48615\n",
      "epoch: 126 loss: 0.46294\n",
      "epoch: 127 loss: 0.44543\n",
      "epoch: 128 loss: 0.41689\n",
      "epoch: 129 loss: 0.38697\n",
      "epoch: 130 loss: 0.38991\n",
      "epoch: 131 loss: 0.36754\n",
      "epoch: 132 loss: 0.32371\n",
      "epoch: 133 loss: 0.30861\n",
      "epoch: 134 loss: 0.31518\n",
      "epoch: 135 loss: 0.28165\n",
      "epoch: 136 loss: 0.30309\n",
      "epoch: 137 loss: 0.50926\n",
      "epoch: 138 loss: 0.50041\n",
      "epoch: 139 loss: 0.49314\n",
      "epoch: 140 loss: 0.46351\n",
      "epoch: 141 loss: 0.46255\n",
      "epoch: 142 loss: 0.45638\n",
      "epoch: 143 loss: 0.46595\n",
      "epoch: 144 loss: 0.45675\n",
      "epoch: 145 loss: 0.44707\n",
      "epoch: 146 loss: 0.4423\n",
      "epoch: 147 loss: 0.47473\n",
      "epoch: 148 loss: 0.44766\n",
      "epoch: 149 loss: 0.45844\n",
      "epoch: 150 loss: 0.46123\n",
      "epoch: 151 loss: 0.43929\n",
      "epoch: 152 loss: 0.41931\n",
      "epoch: 153 loss: 0.42265\n",
      "epoch: 154 loss: 0.42125\n",
      "epoch: 155 loss: 0.41007\n",
      "epoch: 156 loss: 0.39213\n",
      "epoch: 157 loss: 0.39037\n",
      "epoch: 158 loss: 0.38729\n",
      "epoch: 159 loss: 0.38081\n",
      "epoch: 160 loss: 0.37641\n",
      "epoch: 161 loss: 0.38363\n",
      "epoch: 162 loss: 0.35937\n",
      "epoch: 163 loss: 0.33744\n",
      "epoch: 164 loss: 0.34964\n",
      "epoch: 165 loss: 0.33799\n",
      "epoch: 166 loss: 0.35004\n",
      "epoch: 167 loss: 0.36427\n",
      "epoch: 168 loss: 0.3775\n",
      "epoch: 169 loss: 0.36687\n",
      "epoch: 170 loss: 0.35979\n",
      "epoch: 171 loss: 0.38154\n",
      "epoch: 172 loss: 0.35116\n",
      "epoch: 173 loss: 0.33335\n",
      "epoch: 174 loss: 0.36148\n",
      "epoch: 175 loss: 0.33615\n",
      "epoch: 176 loss: 0.31499\n",
      "epoch: 177 loss: 0.31523\n",
      "epoch: 178 loss: 0.2929\n",
      "epoch: 179 loss: 0.28567\n",
      "epoch: 180 loss: 0.29003\n",
      "epoch: 181 loss: 0.28659\n",
      "epoch: 182 loss: 0.28641\n",
      "epoch: 183 loss: 0.2759\n",
      "epoch: 184 loss: 0.27037\n",
      "epoch: 185 loss: 0.23683\n",
      "epoch: 186 loss: 0.26686\n",
      "epoch: 187 loss: 0.27844\n",
      "epoch: 188 loss: 0.26193\n",
      "epoch: 189 loss: 0.25753\n",
      "epoch: 190 loss: 0.23752\n",
      "epoch: 191 loss: 0.24619\n",
      "epoch: 192 loss: 0.25252\n",
      "epoch: 193 loss: 0.27423\n",
      "epoch: 194 loss: 0.23046\n",
      "epoch: 195 loss: 0.22769\n",
      "epoch: 196 loss: 0.22408\n",
      "epoch: 197 loss: 0.20322\n",
      "epoch: 198 loss: 0.22647\n",
      "epoch: 199 loss: 0.21458\n",
      "epoch: 200 loss: 0.19088\n",
      "epoch: 201 loss: 0.21778\n",
      "epoch: 202 loss: 0.19773\n",
      "epoch: 203 loss: 0.19924\n",
      "epoch: 204 loss: 0.19974\n",
      "epoch: 205 loss: 0.19837\n",
      "epoch: 206 loss: 0.18861\n",
      "epoch: 207 loss: 0.17755\n",
      "epoch: 208 loss: 0.19173\n",
      "epoch: 209 loss: 0.19952\n",
      "epoch: 210 loss: 0.20174\n",
      "epoch: 211 loss: 0.18388\n",
      "epoch: 212 loss: 0.20636\n",
      "epoch: 213 loss: 0.16607\n",
      "epoch: 214 loss: 0.20475\n",
      "epoch: 215 loss: 0.18387\n",
      "epoch: 216 loss: 0.17613\n",
      "epoch: 217 loss: 0.1715\n",
      "epoch: 218 loss: 0.17222\n",
      "epoch: 219 loss: 0.16497\n",
      "epoch: 220 loss: 0.17146\n",
      "epoch: 221 loss: 0.16509\n",
      "epoch: 222 loss: 0.15022\n",
      "epoch: 223 loss: 0.16386\n",
      "epoch: 224 loss: 0.16345\n",
      "epoch: 225 loss: 0.1529\n",
      "epoch: 226 loss: 0.16316\n",
      "epoch: 227 loss: 0.18178\n",
      "epoch: 228 loss: 0.1654\n",
      "epoch: 229 loss: 0.14809\n",
      "epoch: 230 loss: 0.14014\n",
      "epoch: 231 loss: 0.14386\n",
      "epoch: 232 loss: 0.14458\n",
      "epoch: 233 loss: 0.1404\n",
      "epoch: 234 loss: 0.14015\n",
      "epoch: 235 loss: 0.14793\n",
      "epoch: 236 loss: 0.18676\n",
      "epoch: 237 loss: 0.14898\n",
      "epoch: 238 loss: 0.14364\n",
      "epoch: 239 loss: 0.14292\n",
      "epoch: 240 loss: 0.13613\n",
      "epoch: 241 loss: 0.12194\n",
      "epoch: 242 loss: 0.12352\n",
      "epoch: 243 loss: 0.13444\n",
      "epoch: 244 loss: 0.14207\n",
      "epoch: 245 loss: 0.1439\n",
      "epoch: 246 loss: 0.132\n",
      "epoch: 247 loss: 0.12693\n",
      "epoch: 248 loss: 0.12721\n",
      "epoch: 249 loss: 0.1333\n",
      "epoch: 250 loss: 0.12266\n",
      "epoch: 251 loss: 0.12956\n",
      "epoch: 252 loss: 0.12322\n",
      "epoch: 253 loss: 0.12338\n",
      "epoch: 254 loss: 0.15147\n",
      "epoch: 255 loss: 0.11548\n",
      "epoch: 256 loss: 0.13643\n",
      "epoch: 257 loss: 0.12977\n",
      "epoch: 258 loss: 0.13389\n",
      "epoch: 259 loss: 0.11429\n",
      "epoch: 260 loss: 0.11051\n",
      "epoch: 261 loss: 0.11726\n",
      "epoch: 262 loss: 0.14788\n",
      "epoch: 263 loss: 0.10372\n",
      "epoch: 264 loss: 0.10163\n",
      "epoch: 265 loss: 0.11376\n",
      "epoch: 266 loss: 0.11064\n",
      "epoch: 267 loss: 0.11272\n",
      "epoch: 268 loss: 0.1182\n",
      "epoch: 269 loss: 0.10659\n",
      "epoch: 270 loss: 0.10001\n",
      "epoch: 271 loss: 0.11137\n",
      "epoch: 272 loss: 0.089716\n",
      "epoch: 273 loss: 0.077253\n",
      "epoch: 274 loss: 0.097808\n",
      "epoch: 275 loss: 0.10962\n",
      "epoch: 276 loss: 0.092138\n",
      "epoch: 277 loss: 0.094917\n",
      "epoch: 278 loss: 0.10083\n",
      "epoch: 279 loss: 0.08366\n",
      "epoch: 280 loss: 0.091843\n",
      "epoch: 281 loss: 0.1076\n",
      "epoch: 282 loss: 0.087153\n",
      "epoch: 283 loss: 0.086155\n",
      "epoch: 284 loss: 0.090331\n",
      "epoch: 285 loss: 0.083164\n",
      "epoch: 286 loss: 0.085224\n",
      "epoch: 287 loss: 0.082746\n",
      "epoch: 288 loss: 0.080484\n",
      "epoch: 289 loss: 0.099796\n",
      "epoch: 290 loss: 0.091578\n",
      "epoch: 291 loss: 0.087484\n",
      "epoch: 292 loss: 0.090508\n",
      "epoch: 293 loss: 0.10027\n",
      "epoch: 294 loss: 0.088802\n",
      "epoch: 295 loss: 0.079317\n",
      "epoch: 296 loss: 0.074141\n",
      "epoch: 297 loss: 0.090613\n",
      "epoch: 298 loss: 0.076322\n",
      "epoch: 299 loss: 0.085761\n",
      "epoch: 300 loss: 0.083735\n",
      "epoch: 301 loss: 0.080883\n",
      "epoch: 302 loss: 0.08354\n",
      "epoch: 303 loss: 0.09486\n",
      "epoch: 304 loss: 0.08986\n",
      "epoch: 305 loss: 0.069695\n",
      "epoch: 306 loss: 0.078306\n",
      "epoch: 307 loss: 0.090543\n",
      "epoch: 308 loss: 0.075876\n",
      "epoch: 309 loss: 0.091409\n",
      "epoch: 310 loss: 0.063063\n",
      "epoch: 311 loss: 0.07869\n",
      "epoch: 312 loss: 0.081153\n",
      "epoch: 313 loss: 0.065635\n",
      "epoch: 314 loss: 0.068293\n",
      "epoch: 315 loss: 0.082244\n",
      "epoch: 316 loss: 0.067885\n",
      "epoch: 317 loss: 0.069539\n",
      "epoch: 318 loss: 0.070352\n",
      "epoch: 319 loss: 0.079939\n",
      "epoch: 320 loss: 0.072836\n",
      "epoch: 321 loss: 0.079017\n",
      "epoch: 322 loss: 0.077807\n",
      "epoch: 323 loss: 0.068726\n",
      "epoch: 324 loss: 0.073924\n",
      "epoch: 325 loss: 0.079765\n",
      "epoch: 326 loss: 0.079552\n",
      "epoch: 327 loss: 0.078437\n",
      "epoch: 328 loss: 0.073965\n",
      "epoch: 329 loss: 0.067721\n",
      "epoch: 330 loss: 0.099085\n",
      "epoch: 331 loss: 0.070827\n",
      "epoch: 332 loss: 0.072973\n",
      "epoch: 333 loss: 0.084526\n",
      "epoch: 334 loss: 0.08375\n",
      "epoch: 335 loss: 0.066742\n",
      "epoch: 336 loss: 0.053357\n",
      "epoch: 337 loss: 0.066461\n",
      "epoch: 338 loss: 0.067998\n",
      "epoch: 339 loss: 0.067867\n",
      "epoch: 340 loss: 0.065886\n",
      "epoch: 341 loss: 0.067303\n",
      "epoch: 342 loss: 0.071166\n",
      "epoch: 343 loss: 0.056588\n",
      "epoch: 344 loss: 0.064249\n",
      "epoch: 345 loss: 0.076246\n",
      "epoch: 346 loss: 0.05829\n",
      "epoch: 347 loss: 0.070347\n",
      "epoch: 348 loss: 0.069179\n",
      "epoch: 349 loss: 0.068056\n",
      "epoch: 350 loss: 0.059879\n",
      "epoch: 351 loss: 0.059704\n",
      "epoch: 352 loss: 0.060156\n",
      "epoch: 353 loss: 0.061905\n",
      "epoch: 354 loss: 0.10147\n",
      "epoch: 355 loss: 0.065007\n",
      "epoch: 356 loss: 0.049772\n",
      "epoch: 357 loss: 0.062527\n",
      "epoch: 358 loss: 0.066068\n",
      "epoch: 359 loss: 0.066728\n",
      "epoch: 360 loss: 0.060386\n",
      "epoch: 361 loss: 0.053715\n",
      "epoch: 362 loss: 0.051065\n",
      "epoch: 363 loss: 0.052489\n",
      "epoch: 364 loss: 0.061718\n",
      "epoch: 365 loss: 0.071847\n",
      "epoch: 366 loss: 0.068326\n",
      "epoch: 367 loss: 0.058305\n",
      "epoch: 368 loss: 0.064123\n",
      "epoch: 369 loss: 0.049815\n",
      "epoch: 370 loss: 0.065155\n",
      "epoch: 371 loss: 0.064396\n",
      "epoch: 372 loss: 0.063236\n",
      "epoch: 373 loss: 0.05344\n",
      "epoch: 374 loss: 0.079889\n",
      "epoch: 375 loss: 0.052942\n",
      "epoch: 376 loss: 0.058405\n",
      "epoch: 377 loss: 0.060045\n",
      "epoch: 378 loss: 0.05577\n",
      "epoch: 379 loss: 0.037791\n",
      "epoch: 380 loss: 0.052117\n",
      "epoch: 381 loss: 0.051072\n",
      "epoch: 382 loss: 0.047352\n",
      "epoch: 387 loss: 0.061457\n",
      "epoch: 388 loss: 0.053843\n",
      "epoch: 389 loss: 0.052122\n",
      "epoch: 390 loss: 0.055836\n",
      "epoch: 391 loss: 0.097287\n",
      "epoch: 392 loss: 0.055851\n",
      "epoch: 393 loss: 0.064806\n",
      "epoch: 394 loss: 0.054906\n",
      "epoch: 395 loss: 0.068496\n",
      "epoch: 396 loss: 0.053919\n",
      "epoch: 397 loss: 0.048254\n",
      "epoch: 398 loss: 0.055911\n",
      "epoch: 399 loss: 0.051502\n",
      "epoch: 400 loss: 0.06601\n",
      "epoch: 401 loss: 0.0508\n",
      "epoch: 402 loss: 0.061765\n",
      "epoch: 403 loss: 0.055015\n",
      "epoch: 404 loss: 0.05016\n",
      "epoch: 405 loss: 0.031271\n",
      "epoch: 406 loss: 0.043662\n",
      "epoch: 407 loss: 0.048495\n",
      "epoch: 408 loss: 0.061754\n",
      "epoch: 409 loss: 0.037903\n",
      "epoch: 410 loss: 0.049995\n",
      "epoch: 411 loss: 0.05266\n",
      "epoch: 412 loss: 0.076148\n",
      "epoch: 414 loss: 0.041002\n",
      "epoch: 415 loss: 0.04491\n",
      "epoch: 416 loss: 0.045092\n",
      "epoch: 417 loss: 0.044771\n",
      "epoch: 418 loss: 0.04555\n",
      "epoch: 419 loss: 0.054696\n",
      "epoch: 420 loss: 0.036161\n",
      "epoch: 421 loss: 0.039369\n",
      "epoch: 422 loss: 0.044004\n",
      "epoch: 423 loss: 0.040142\n",
      "epoch: 424 loss: 0.05786\n",
      "epoch: 425 loss: 0.039071\n",
      "epoch: 426 loss: 0.07384\n",
      "epoch: 427 loss: 0.043589\n",
      "epoch: 428 loss: 0.041379\n",
      "epoch: 429 loss: 0.050442\n",
      "epoch: 430 loss: 0.039172\n",
      "epoch: 431 loss: 0.064721\n",
      "epoch: 432 loss: 0.050519\n",
      "epoch: 433 loss: 0.034249\n",
      "epoch: 434 loss: 0.051467\n",
      "epoch: 435 loss: 0.050566\n",
      "epoch: 436 loss: 0.05906\n",
      "epoch: 437 loss: 0.049479\n",
      "epoch: 438 loss: 0.098575\n",
      "epoch: 439 loss: 0.088464\n",
      "epoch: 440 loss: 0.092972\n",
      "epoch: 441 loss: 0.074183\n",
      "epoch: 442 loss: 0.077779\n",
      "epoch: 443 loss: 0.074119\n",
      "epoch: 444 loss: 0.060629\n",
      "epoch: 445 loss: 0.12952\n",
      "epoch: 446 loss: 0.14511\n",
      "epoch: 447 loss: 0.29187\n",
      "epoch: 448 loss: 0.13965\n",
      "epoch: 449 loss: 0.10192\n",
      "epoch: 450 loss: 0.096169\n",
      "epoch: 451 loss: 0.084602\n",
      "epoch: 452 loss: 0.072717\n",
      "epoch: 453 loss: 0.094238\n",
      "epoch: 454 loss: 0.058621\n",
      "epoch: 455 loss: 0.060396\n",
      "epoch: 456 loss: 0.05032\n",
      "epoch: 457 loss: 0.056229\n",
      "epoch: 458 loss: 0.05734\n",
      "epoch: 459 loss: 0.063152\n",
      "epoch: 460 loss: 0.052786\n",
      "epoch: 461 loss: 0.06517\n",
      "epoch: 462 loss: 0.043362\n",
      "epoch: 463 loss: 0.061493\n",
      "epoch: 464 loss: 0.048823\n",
      "epoch: 465 loss: 0.037896\n",
      "epoch: 466 loss: 0.048399\n",
      "epoch: 467 loss: 0.044688\n",
      "epoch: 468 loss: 0.048115\n",
      "epoch: 469 loss: 0.050166\n",
      "epoch: 470 loss: 0.045506\n",
      "epoch: 471 loss: 0.04754\n",
      "epoch: 472 loss: 0.041357\n",
      "epoch: 473 loss: 0.037915\n",
      "epoch: 474 loss: 0.045234\n",
      "epoch: 475 loss: 0.040611\n",
      "epoch: 476 loss: 0.044418\n",
      "epoch: 477 loss: 0.04856\n",
      "epoch: 478 loss: 0.042726\n",
      "epoch: 479 loss: 0.054039\n",
      "epoch: 480 loss: 0.052199\n",
      "epoch: 481 loss: 0.031637\n",
      "epoch: 482 loss: 0.029916\n",
      "epoch: 483 loss: 0.041724\n",
      "epoch: 484 loss: 0.054322\n",
      "epoch: 485 loss: 0.058825\n",
      "epoch: 486 loss: 0.044609\n",
      "epoch: 487 loss: 0.032857\n",
      "epoch: 488 loss: 0.046852\n",
      "epoch: 489 loss: 0.044286\n",
      "epoch: 495 loss: 0.040579\n",
      "epoch: 496 loss: 0.028495\n",
      "epoch: 497 loss: 0.029749\n",
      "epoch: 498 loss: 0.040918\n",
      "epoch: 499 loss: 0.03269\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda'\n",
    "# with t.cuda.device(0):\n",
    "lr = 3e-4\n",
    "optimizer = t.optim.Adam(tiny_bert.parameters(), lr = lr)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "tiny_bert.train()\n",
    "tiny_bert.to(device)\n",
    "\n",
    "for epoch in range(100, 500):\n",
    "    for masked_sentences, masks, unmasked_sentences in train_dataloader:\n",
    "        gradient_steps += 1\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        masked_sentences = masked_sentences.to(device)\n",
    "        masks = masks.to(device)\n",
    "        unmasked_sentences = unmasked_sentences.to(device)\n",
    "\n",
    "        word_logits = tiny_bert(masked_sentences.to(device))\n",
    "\n",
    "        masks = masks.to(torch.bool)\n",
    "\n",
    "        # word_logits.shape: batch_size, seq_length, vocab_size\n",
    "        # mask.shape = batch_size, seq_length\n",
    "        # Mask the logits\n",
    "        masked_logits = torch.masked_select(word_logits, masks.unsqueeze(-1))\n",
    "\n",
    "        # Reshaping to work with loss function\n",
    "        masked_logits = einops.rearrange(masked_logits, '(num_masks vocab_size) -> num_masks vocab_size', \n",
    "                                         vocab_size=vocab_size)\n",
    "        target = torch.masked_select(unmasked_sentences, masks)\n",
    "\n",
    "        # Averaging the loss across the sequences\n",
    "        batch_size = masked_sentences.shape[0]\n",
    "        loss = loss_fn(masked_logits, target) / batch_size\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"epoch: {epoch}\", f\"loss: {loss.item():.5}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([4., 5., 6., 4., 5., 6.])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = t.Tensor([[1, 2, 3], [4, 5, 6], [4, 5, 6]])\n",
    "t.masked_select(a, t.tensor([0,1, 1], dtype=t.bool).unsqueeze(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataloader = get_wiki_batches(test_wiki_data, 16, 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 64, 28996])\n",
      "INPUTS      [CLS] The [MASK] earned the Bulldogs a trip back to Indianapolis for the first Final [MASK] appearance [MASK] school and Horizon League performed. The win made Stevens, at [MASK] 33 [MASK] the youngest coach to lead a team to the Final Four since Bob Knight made his [MASK] Final Four appearance at age [MASK] in 1973. Butler became the [MASK] school\n",
      "PREDICTIONS [CLS] The show started the had a set back to continued for the social daydale appearance had Florida and had T City. The day the recording, at thero for the S'to be a team to the position singles since men Upon made his plan international would appearance at four game in 15. An became the five At\n",
      "ACTUAL      [CLS] The win earned the Bulldogs a trip back to Indianapolis for the first Final Four appearance in school and Horizon League history. The win made Stevens, at age 33, the youngest coach to lead a team to the Final Four since Bob Knight made his first Final Four appearance at age 32 in 1973. Butler became the smallest school\n",
      "\n",
      "INPUTS      [CLS] Horizon League Coach of the Year ( 2009 [MASK] 2010 [MASK] [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "PREDICTIONS [CLS] The Somerset time of the September ( [UNK] ) ). [SEP] ) ) ) ) â ) ) ) the ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) and ) rounds ) ) the ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) ) @\n",
      "ACTUAL      [CLS] Horizon League Coach of the Year ( 2009, 2010 ) [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "\n",
      "INPUTS      [CLS] The 1st [MASK] as 2015 population [MASK] 415 @, @ [UNK] ) is Manila's [MASK] [MASK] the country [MASK] pronunciation ) most densely populated congressional district. ã covers the western portion of [UNK] that lies along Manila Bay. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "PREDICTIONS [CLS] The brigademe as most ( / - @, @ [UNK] ) is Guitar's variant of the country acid Re ) [UNK] (el the East. Other in thedox transmission of [UNK] that Sa the the C. [SEP] m verse m m m Me floor verseing 1st floor Forces Forces Forces m Mexico verse home verse\n",
      "ACTUAL      [CLS] The 1st District ( 2015 population : 415 @, @ [UNK] ) is Manila's ( and the country's ) most densely populated congressional district. It covers the western portion of [UNK] that lies along Manila Bay. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "\n",
      "INPUTS      [CLS] Despite the problems, the \" tin [MASK] [MASK], as the [MASK] referred to them, were in active [MASK] from March 1915 through the end of [MASK] war, with half [MASK] the 20 boats lost [MASK] the war. [MASK] [MASK] [MASK] the class served in three navies : the German Imperial Navy [MASK] the Austro @ Jewel [MASK]\n",
      "PREDICTIONS [CLS] At the series, the \" - @ Peninsula, as the baseball referred to law, were in active convoy from Best Twins through the end of the war, with wearing near the famous Iowa moved of the war. After @ of the major scheduled in the season 2013 : the New lowerpiece of the attention @ places.\n",
      "ACTUAL      [CLS] Despite the problems, the \" tin [UNK] \", as the Germans referred to them, were in active service from March 1915 through the end of the war, with half of the 20 boats lost during the war. Boats of the class served in three navies : the German Imperial Navy, the Austro @ - @\n",
      "\n",
      "INPUTS      [CLS] The route initially [MASK] a line of homes as it heads along Robinson Road ; however, it [MASK] enters a [MASK] district surrounding the road [MASK] [MASK] intersection with NY 78 ( [MASK] Road ). At this [MASK], the [UNK] â [UNK] town line turns [MASK] to follow NY 78, therapy NY 93 fully within the town of [UNK]\n",
      "PREDICTIONS [CLS] The route state of a mixture of river as it is â large Road ; Pennsylvania, it can well a very U than the very. Class basketball with NY like ( h Road ). 96 this state, the [UNK] â [UNK] verse river southern mostly to the NY ft, 4 NY (zi across the center of [UNK]\n",
      "ACTUAL      [CLS] The route initially serves a line of homes as it heads along Robinson Road ; however, it soon enters a commercial district surrounding the road's intersection with NY 78 ( Transit Road ). At this point, the [UNK] â [UNK] town line turns south to follow NY 78, leaving NY 93 fully within the town of [UNK]\n",
      "\n",
      "INPUTS      [CLS] At the conclusion of the finals series, the winner of two awards were announced [MASK] Matthew [UNK] won both the Pitcher of the Year award and the Finals Series MVP [MASK]. At the Baseball Australia Diamond Awards, held on 6 March at the Hotel Grand Chancellor, Adelaide, [MASK] [UNK] was announced as the 35th winner of the\n",
      "PREDICTIONS [CLS] At the creation of the commander1, the U of thesis were during includes mushroom [UNK], Enterprise the lowest and of the complete 8 and the latter, the will. At the South. The Gulf, commander on Voyage March at the fourth many settlements, better, the [UNK] was 38 as the Long attacked of the\n",
      "ACTUAL      [CLS] At the conclusion of the finals series, the winner of two awards were announced. Matthew [UNK] won both the Pitcher of the Year award and the Finals Series MVP award. At the Baseball Australia Diamond Awards, held on 6 March at the Hotel Grand Chancellor, Adelaide, Wayne [UNK] was announced as the 35th winner of the\n",
      "\n",
      "INPUTS      [CLS] [MASK] criticized Donald's controls in certain [MASK] in [MASK] game [MASK] as well as the difficulty of [MASK] levels and puzzles. Buchanan [MASK] criticized the [MASK], calling them [MASK] float [MASK] - @ y \" and noted the difficulty in executing precision jumps, explaining that Grimes [ i ] t's Apache too easy [MASK] [UNK] deadly\n",
      "PREDICTIONS [CLS] The the under's pre in Australian available in Los game, as well as the crowd of accompaniment game and strategy. June indoor called the children, being. the included @ - @ flight \" and the theor in popularma policy, there that eight. time'his's'action to to [UNK] These\n",
      "ACTUAL      [CLS] [UNK] criticized Donald's controls in certain situations in the game, as well as the difficulty of some levels and puzzles. Buchanan also criticized the controls, calling them \" float @ - @ y \" and noted the difficulty in executing precision jumps, explaining that \" [ i ] t's far too easy to [UNK] or\n",
      "\n",
      "INPUTS      [CLS] = = = National government = = = [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "PREDICTIONS [CLS] = = = John journey = = = [SEP] = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = =\n",
      "ACTUAL      [CLS] = = = National government = = = [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "\n",
      "INPUTS      [MASK] A neutron [MASK] on board the [UNK] [UNK] spacecraft detected [MASK] concentrations of [MASK] close to the northern and southern lunar poles, [MASK] the [MASK] [UNK]. At the end of this mission in July 1999, the spacecraft was crashed into [MASK] nearby [MASK] [MASK]hoemaker in the hope of detecting from Earth @ - @ based telescopes\n",
      "PREDICTIONS [CLS] A April opened on the the [UNK] [UNK] continued.ru end of the innovative to the Minnesota andvo Caribbean century, around the WA [UNK]. At the end of this capital in the Airport, the route was acceptable into the better. stone highway BCE in the coast oftring from the @ - @ Normanlas\n",
      "ACTUAL      [CLS] A neutron [UNK] on board the [UNK] [UNK] spacecraft detected enhanced concentrations of hydrogen close to the northern and southern lunar poles, including the crater [UNK]. At the end of this mission in July 1999, the spacecraft was crashed into the nearby crater Shoemaker in the hope of detecting from Earth @ - @ based telescopes\n",
      "\n",
      "INPUTS      [CLS] Popular [MASK] destinations in [MASK] include [MASK] [UNK] de San Diego [MASK] [MASK] [UNK] Golf Course, [UNK] de Santa Lucia [MASK] Fort Santiago, Manila Cathedral, [UNK] [UNK], [MASK] de Santa [UNK], [UNK] [MASK] [UNK], Plaza Mexico, Plaza de [UNK], San Agustin Church and the [UNK] de Manila. [MASK] [PAD] [PAD] [PAD] [PAD]\n",
      "PREDICTIONS [CLS] A G also in theal An [UNK] de privatey assigned, [UNK] like those, [UNK]eluo de Forts, Ree, [UNK] [UNK], Gus are [UNK], [UNK] de [UNK], Johny, Utah de [UNK], San g expressiontinv and the [UNK] told [UNK]. [SEP] [SEP] [SEP] = [SEP]\n",
      "ACTUAL      [CLS] Popular tourist destinations in [UNK] include the [UNK] de San Diego, Club [UNK] Golf Course, [UNK] de Santa Lucia, Fort Santiago, Manila Cathedral, [UNK] [UNK], [UNK] de Santa [UNK], [UNK] del [UNK], Plaza Mexico, Plaza de [UNK], San Agustin Church and the [UNK] de Manila. [SEP] [PAD] [PAD] [PAD] [PAD]\n",
      "\n",
      "INPUTS      [MASK] Napoleon's lightning campaign exposed the Austrian [UNK] command structure and poor supply apparatus. Mack completely [UNK] the French [UNK] and scattered [MASK] [MASK] ; as the French defeated each [MASK] separately, the surviving Austrians withdrew toward the Ulm fortifications [MASK] reappeared arrived to take personal command of close [MASK] 80 @ [MASK] @ 000 men. At\n",
      "PREDICTIONS [CLS] Fowler's system the by the Georg [UNK] command heat and since the road. areed [UNK] the second [UNK] and troops, level ; as the boats [UNK] based. area, the. systems of of the U across rubbing lower our river to looking throughout area of [CLS] to Cal @ - @ 000 line. At\n",
      "ACTUAL      [CLS] Napoleon's lightning campaign exposed the Austrian [UNK] command structure and poor supply apparatus. Mack completely [UNK] the French [UNK] and scattered his forces ; as the French defeated each unit separately, the surviving Austrians withdrew toward the Ulm fortifications. Napoleon arrived to take personal command of close to 80 @, @ 000 men. At\n",
      "\n",
      "INPUTS      [CLS] The Portage to San [UNK] of [UNK] is a 1981 [MASK] and philosophical novella [MASK] George Steiner, in which Jewish Nazi hunters find a fictional Adolf Hitler ( [MASK] ) alive in the Amazon jungle thirty years after the end of World War II. The book generated considerable controversy [MASK] its publication because in it, Steiner, who is\n",
      "PREDICTIONS [CLS] Thetin Poland to use [UNK] of [UNK] is a true, andre of of mushroom city, in which white eastern province like a peak (. ( [UNK] ) ) in the v, affected species in the end of World War II. The is Dream wholen of its publication well in it, but, who is\n",
      "ACTUAL      [CLS] The Portage to San [UNK] of [UNK] is a 1981 literary and philosophical novella by George Steiner, in which Jewish Nazi hunters find a fictional Adolf Hitler ( [UNK] ) alive in the Amazon jungle thirty years after the end of World War II. The book generated considerable controversy after its publication because in it, Steiner, who is\n",
      "\n",
      "INPUTS      [CLS] Glo [MASK]'s [MASK] hull was modelled [MASK] that [MASK] a steam [MASK] [MASK] [MASK] line, reduced to one deck, [UNK] in [MASK] [MASK] 4 [MASK]. @ 5 inches ( [MASK] mm ) thick. She was propelled by a steam engine, driving [MASK] single screw propeller for a speed of 13 knots ( 24 km /\n",
      "PREDICTIONS [CLS] Several armouredpole's Human there was a ) that over a way of @ the Corporation, G to United Alfred, [UNK] in the the 4ust. @ g known ( Jo through ) BC. She was shown by a greater military, way. Old baseball images for a diagonal of Witch 1990 ( 56 km /\n",
      "ACTUAL      [CLS] Gloire's wooden hull was modelled on that of a steam ship of the line, reduced to one deck, [UNK] in iron plates 4 @. @ 5 inches ( 110 mm ) thick. She was propelled by a steam engine, driving a single screw propeller for a speed of 13 knots ( 24 km /\n",
      "\n",
      "INPUTS      [CLS] In mid @ - @ November, [MASK] J [MASK] briefly tracked a weak tropical depression [MASK] Wake Island. The agency also briefly tracked a tropical depression off the coast of Vietnam on December 16. It finally dissipated [MASK] December 17, [MASK] the pressure and winds [MASK]. The final system [MASK] the year was a tropical depression [MASK] originated\n",
      "PREDICTIONS [CLS] In York @ - @ wave, @ existed and @ slowly a which tropical depression of since coast. The United also river outer a tropical Wave Miami the coast of the on Gulf Chart. It finally Gulf. December level, near the next and coast day. The highway formed of the year was a tropical depression of strengthened\n",
      "ACTUAL      [CLS] In mid @ - @ November, the JMA briefly tracked a weak tropical depression near Wake Island. The agency also briefly tracked a tropical depression off the coast of Vietnam on December 16. It finally dissipated on December 17, with the pressure and winds unknown. The final system of the year was a tropical depression that originated\n",
      "\n",
      "INPUTS      [CLS] Washington, D. Crumbling A replica of San Lorenzo Head 4 sculpted [MASK] [UNK] [UNK] [UNK] was placed near the Constitution Avenue entrance of the Smithsonian [MASK] Museum of Natural History in [MASK] [MASK]. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "PREDICTIONS [CLS] An, Do Cworth A said of the commander since the since against [UNK] [UNK] [UNK] was city near the US act originated of the Although age appearance of pre across in some Sussex. [SEP] l G [UNK] body Haifa five show verse five [UNK] reported roled five [UNK] [UNK] five five aged age ins [UNK]d A\n",
      "ACTUAL      [CLS] Washington, D. C. A replica of San Lorenzo Head 4 sculpted by [UNK] [UNK] [UNK] was placed near the Constitution Avenue entrance of the Smithsonian National Museum of Natural History in October 2001. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "\n",
      "INPUTS      [CLS] After 30 years [MASK] [UNK] and a change in ownership [MASK] the station, his show was replaced with the [UNK] Network's The Larry King Show. In [MASK] 1980s, [UNK] [MASK] [MASK] at Buffalo's [UNK] College and served as [MASK] disc jockey on [MASK] Broadcasting'[MASK]zor station [UNK] [MASK] now [UNK] ). He also\n",
      "PREDICTIONS [CLS] Afteruin, the [UNK] and a based in breakthrough the the final, his show was well with the [UNK]2's The publisher the destroyed. In the 1985, [UNK] [UNK] the ; Olivier's [UNK] needs and satisfied as the by, on the Man's was [UNK] [UNK] [UNK], [UNK] ). He Olivier\n",
      "ACTUAL      [CLS] After 30 years with [UNK] and a change in ownership for the station, his show was replaced with the [UNK] Network's The Larry King Show. In the 1980s, [UNK] taught communications at Buffalo's [UNK] College and served as a disc jockey on Public Broadcasting's radio station [UNK] ( now [UNK] ). He also\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tiny_bert.eval()\n",
    "for masked_sentences, masks, unmasked_sentences in test_dataloader:    \n",
    "    masked_sentences = masked_sentences.to(device)\n",
    "    masks = masks.to(torch.bool)\n",
    "    masks = masks.to(device)\n",
    "    unmasked_sentences = unmasked_sentences.to(device)\n",
    "\n",
    "    word_logits = tiny_bert(masked_sentences.to(device))\n",
    "    print(word_logits.shape)\n",
    "    predictions = torch.argmax(word_logits, dim=-1)\n",
    "    for i in range(len(predictions)):\n",
    "        print(\"INPUTS     \", tokenizer.decode(masked_sentences[i]))\n",
    "        print(\"PREDICTIONS\", tokenizer.decode(predictions[i]))\n",
    "        print(\"ACTUAL     \", tokenizer.decode(unmasked_sentences[i]))\n",
    "        print()\n",
    "    break\n",
    " \n",
    "    # word_logits.shape: batch_size, seq_length, vocab_size\n",
    "#     # mask.shape = batch_size, seq_length\n",
    "#     # Mask the logits\n",
    "#     masked_logits = torch.masked_select(word_logits, masks.unsqueeze(-1))\n",
    "    \n",
    "#     # Reshaping to work with loss function\n",
    "#     masked_logits = einops.rearrange(masked_logits, '(num_masks vocab_size) -> num_masks vocab_size', \n",
    "#                                      vocab_size=vocab_size)\n",
    "#     target = torch.masked_select(unmasked_sentences, masks)\n",
    "\n",
    "#     # Averaging the loss across the sequences\n",
    "#     batch_size = masked_sentences.shape[0]\n",
    "#     loss = loss_fn(masked_logits, target) / batch_size\n",
    "        \n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "#     if gradient_steps % 20 == 0:\n",
    "#         print(gradient_steps, loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 64, 28996])\n",
      "INPUTS      [CLS] The [MASK] earned the Bulldogs a trip back to Indianapolis for the first Final [MASK] appearance [MASK] school and Horizon League performed. The win made Stevens, at [MASK] 33 [MASK] the youngest coach to lead a team to the Final Four since Bob Knight made his [MASK] Final Four appearance at age [MASK] in 1973. Butler became the [MASK] school\n",
      "PREDICTIONS [CLS] The new performed the record a A began to record for the first the ( American and record and American Division record. The 9 four Cup, at the 9 in the head continued with make a Star with the the Tech American the record with the record the Tech 1995 the American as in record. record became the new record\n",
      "ACTUAL      [CLS] The win earned the Bulldogs a trip back to Indianapolis for the first Final Four appearance in school and Horizon League history. The win made Stevens, at age 33, the youngest coach to lead a team to the Final Four since Bob Knight made his first Final Four appearance at age 32 in 1973. Butler became the smallest school\n",
      "\n",
      "INPUTS      [CLS] Horizon League Coach of the Year ( 2009 [MASK] 2010 [MASK] [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "PREDICTIONS [CLS] American ) ) of the ) and ) ) ) ) [SEP] ) ) ) ) the, the the the ) ) ) ) and ) the the ) ) the a ), in and ) ) ) and ) ). the ) the the and ) and the.. the and ) ) the. and in and\n",
      "ACTUAL      [CLS] Horizon League Coach of the Year ( 2009, 2010 ) [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "\n",
      "INPUTS      [CLS] The 1st [MASK] as 2015 population [MASK] 415 @, @ [UNK] ) is Manila's [MASK] [MASK] the country [MASK] pronunciation ) most densely populated congressional district. ã covers the western portion of [UNK] that lies along Manila Bay. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "PREDICTIONS [CLS] The contains production as the ( the - @, @ [UNK] It is Peter's period to the production p north and a in : the city. It of the the lack of [UNK] is ( the like north. [SEP] of city the the, States the north 1995. long 1995 the north population long of the the\n",
      "ACTUAL      [CLS] The 1st District ( 2015 population : 415 @, @ [UNK] ) is Manila's ( and the country's ) most densely populated congressional district. It covers the western portion of [UNK] that lies along Manila Bay. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "\n",
      "INPUTS      [CLS] Despite the problems, the \" tin [MASK] [MASK], as the [MASK] referred to them, were in active [MASK] from March 1915 through the end of [MASK] war, with half [MASK] the 20 boats lost [MASK] the war. [MASK] [MASK] [MASK] the class served in three navies : the German Imperial Navy [MASK] the Austro @ Jewel [MASK]\n",
      "PREDICTIONS [CLS] At the government, the and part, day, as the religious longest to side, day in Pennsylvania suspension from Between government through the end of the day, with number of the Not road then of the road. During which in thehara off in three of red at the Germaneach. During the Dutch @ -.\n",
      "ACTUAL      [CLS] Despite the problems, the \" tin [UNK] \", as the Germans referred to them, were in active service from March 1915 through the end of the war, with half of the 20 boats lost during the war. Boats of the class served in three navies : the German Imperial Navy, the Austro @ - @\n",
      "\n",
      "INPUTS      [CLS] The route initially [MASK] a line of homes as it heads along Robinson Road ; however, it [MASK] enters a [MASK] district surrounding the road [MASK] [MASK] intersection with NY 78 ( [MASK] Road ). At this [MASK], the [UNK] â [UNK] town line turns [MASK] to follow NY 78, therapy NY 93 fully within the town of [UNK]\n",
      "PREDICTIONS [CLS] The north along as a north of 4 as it east the 90 ) ; however, it there along a north north along the road north north along with north miles ( 1 [UNK] ). in the north, the [UNK] â [UNK] May of US estimated to north north ), ( local, estimated of the north of [UNK]\n",
      "ACTUAL      [CLS] The route initially serves a line of homes as it heads along Robinson Road ; however, it soon enters a commercial district surrounding the road's intersection with NY 78 ( Transit Road ). At this point, the [UNK] â [UNK] town line turns south to follow NY 78, leaving NY 93 fully within the town of [UNK]\n",
      "\n",
      "INPUTS      [CLS] At the conclusion of the finals series, the winner of two awards were announced [MASK] Matthew [UNK] won both the Pitcher of the Year award and the Finals Series MVP [MASK]. At the Baseball Australia Diamond Awards, held on 6 March at the Hotel Grand Chancellor, Adelaide, [MASK] [UNK] was announced as the 35th winner of the\n",
      "PREDICTIONS [CLS] During the creation of the fifth season, the account of the such were decided : establishment [UNK] published the the writing creation of the fifth point and the local towns were requests. During the last. series year, 8 on 8 fifth at the ( local performances, Congress, the [UNK] was such as the local development of the\n",
      "ACTUAL      [CLS] At the conclusion of the finals series, the winner of two awards were announced. Matthew [UNK] won both the Pitcher of the Year award and the Finals Series MVP award. At the Baseball Australia Diamond Awards, held on 6 March at the Hotel Grand Chancellor, Adelaide, Wayne [UNK] was announced as the 35th winner of the\n",
      "\n",
      "INPUTS      [CLS] [MASK] criticized Donald's controls in certain [MASK] in [MASK] game [MASK] as well as the difficulty of [MASK] levels and puzzles. Buchanan [MASK] criticized the [MASK], calling them [MASK] float [MASK] - @ y \" and noted the difficulty in executing precision jumps, explaining that Grimes [ i ] t's Apache too easy [MASK] [UNK] deadly\n",
      "PREDICTIONS [CLS] The modelr's belief in Alice known of the list as sent well as the groups of red letters and together. model, during the city, now attention as Ohio @ - @, \" and along the model in others of policy displays model, Michael, and and \"'s as as groups as [UNK] critical\n",
      "ACTUAL      [CLS] [UNK] criticized Donald's controls in certain situations in the game, as well as the difficulty of some levels and puzzles. Buchanan also criticized the controls, calling them \" float @ - @ y \" and noted the difficulty in executing precision jumps, explaining that \" [ i ] t's far too easy to [UNK] or\n",
      "\n",
      "INPUTS      [CLS] = = = National government = = = [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "PREDICTIONS [CLS] = = = German government = = = [SEP] = Italian = = = Peter [SEP] = = Italian = = = = = = = = [SEP] = = = = broke = Spain = = = = = = = = Italian = = = = = = = = = = German = = = Italian = Italian = =\n",
      "ACTUAL      [CLS] = = = National government = = = [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "\n",
      "INPUTS      [MASK] A neutron [MASK] on board the [UNK] [UNK] spacecraft detected [MASK] concentrations of [MASK] close to the northern and southern lunar poles, [MASK] the [MASK] [UNK]. At the end of this mission in July 1999, the spacecraft was crashed into [MASK] nearby [MASK] [MASK]hoemaker in the hope of detecting from Earth @ - @ based telescopes\n",
      "PREDICTIONS [CLS] ASPde on at the [UNK] [UNK] Thomas common the [UNK] of the discovered to the [UNK] and Byzantine argued [UNK], along the road [UNK]. During the end of this scene in thements, the route was swept into about supposedem re em directly in the range of 2009 was from the @ - @ [UNK] As\n",
      "ACTUAL      [CLS] A neutron [UNK] on board the [UNK] [UNK] spacecraft detected enhanced concentrations of hydrogen close to the northern and southern lunar poles, including the crater [UNK]. At the end of this mission in July 1999, the spacecraft was crashed into the nearby crater Shoemaker in the hope of detecting from Earth @ - @ based telescopes\n",
      "\n",
      "INPUTS      [CLS] Popular [MASK] destinations in [MASK] include [MASK] [UNK] de San Diego [MASK] [MASK] [UNK] Golf Course, [UNK] de Santa Lucia [MASK] Fort Santiago, Manila Cathedral, [UNK] [UNK], [MASK] de Santa [UNK], [UNK] [MASK] [UNK], Plaza Mexico, Plaza de [UNK], San Agustin Church and the [UNK] de Manila. [MASK] [PAD] [PAD] [PAD] [PAD]\n",
      "PREDICTIONS [CLS] A transportationton in one with commonly [UNK] its San service to King [UNK] s ), [UNK] Center ( last in O ), Somerset ), [UNK] [UNK], strong ( from [UNK], [UNK], [UNK], alongage, Fe of [UNK], San Stton to, and the [UNK] from Syria. [SEP] two of in (\n",
      "ACTUAL      [CLS] Popular tourist destinations in [UNK] include the [UNK] de San Diego, Club [UNK] Golf Course, [UNK] de Santa Lucia, Fort Santiago, Manila Cathedral, [UNK] [UNK], [UNK] de Santa [UNK], [UNK] del [UNK], Plaza Mexico, Plaza de [UNK], San Agustin Church and the [UNK] de Manila. [SEP] [PAD] [PAD] [PAD] [PAD]\n",
      "\n",
      "INPUTS      [MASK] Napoleon's lightning campaign exposed the Austrian [UNK] command structure and poor supply apparatus. Mack completely [UNK] the French [UNK] and scattered [MASK] [MASK] ; as the French defeated each [MASK] separately, the surviving Austrians withdrew toward the Ulm fortifications [MASK] reappeared arrived to take personal command of close [MASK] 80 @ [MASK] @ 000 men. At\n",
      "PREDICTIONS [CLS] Christine's Airport command introduced the close [UNK] their forces and new close military. The fifth of the fifth [UNK] and horses more ; as the more deployment more the @, the the deployments at from the fifth new.. Australia decided to more the end of much four more - - @ close close. During\n",
      "ACTUAL      [CLS] Napoleon's lightning campaign exposed the Austrian [UNK] command structure and poor supply apparatus. Mack completely [UNK] the French [UNK] and scattered his forces ; as the French defeated each unit separately, the surviving Austrians withdrew toward the Ulm fortifications. Napoleon arrived to take personal command of close to 80 @, @ 000 men. At\n",
      "\n",
      "INPUTS      [CLS] The Portage to San [UNK] of [UNK] is a 1981 [MASK] and philosophical novella [MASK] George Steiner, in which Jewish Nazi hunters find a fictional Adolf Hitler ( [MASK] ) alive in the Amazon jungle thirty years after the end of World War II. The book generated considerable controversy [MASK] its publication because in it, Steiner, who is\n",
      "PREDICTIONS [CLS] Theing were to the [UNK] of [UNK] is aer, and modernter ; the background, in which during modern literary during a parents ( [UNK] ( [UNK] ) birds in the modern is Sainting in the end of World modern II. The while was modern ( during its modern [UNK], ), 1940, who is\n",
      "ACTUAL      [CLS] The Portage to San [UNK] of [UNK] is a 1981 literary and philosophical novella by George Steiner, in which Jewish Nazi hunters find a fictional Adolf Hitler ( [UNK] ) alive in the Amazon jungle thirty years after the end of World War II. The book generated considerable controversy after its publication because in it, Steiner, who is\n",
      "\n",
      "INPUTS      [CLS] Glo [MASK]'s [MASK] hull was modelled [MASK] that [MASK] a steam [MASK] [MASK] [MASK] line, reduced to one deck, [UNK] in [MASK] [MASK] 4 [MASK]. @ 5 inches ( [MASK] mm ) thick. She was propelled by a steam engine, driving [MASK] single screw propeller for a speed of 13 knots ( 24 km /\n",
      "PREDICTIONS [CLS] Theapole's first churchyard was an work that became a population primary @ the Company, the to one ), [UNK] in the / ) @. @ 5 9 ( 5 ) ) plate. She was defeated by a broken /, / in 5 broken3 for a star of 13 tons ( / km /\n",
      "ACTUAL      [CLS] Gloire's wooden hull was modelled on that of a steam ship of the line, reduced to one deck, [UNK] in iron plates 4 @. @ 5 inches ( 110 mm ) thick. She was propelled by a steam engine, driving a single screw propeller for a speed of 13 knots ( 24 km /\n",
      "\n",
      "INPUTS      [CLS] In mid @ - @ November, [MASK] J [MASK] briefly tracked a weak tropical depression [MASK] Wake Island. The agency also briefly tracked a tropical depression off the coast of Vietnam on December 16. It finally dissipated [MASK] December 17, [MASK] the pressure and winds [MASK]. The final system [MASK] the year was a tropical depression [MASK] originated\n",
      "PREDICTIONS [CLS] In 20 @ - @ intensified, a day was G through, included included 1935 with landed cyclone. The day also included through a included depression off the coast of slowly on June disturbance. It northwest disturbance, As disturbance, in the 1935, depressions. The highway field, the island with a tropical depression with through\n",
      "ACTUAL      [CLS] In mid @ - @ November, the JMA briefly tracked a weak tropical depression near Wake Island. The agency also briefly tracked a tropical depression off the coast of Vietnam on December 16. It finally dissipated on December 17, with the pressure and winds unknown. The final system of the year was a tropical depression that originated\n",
      "\n",
      "INPUTS      [CLS] Washington, D. Crumbling A replica of San Lorenzo Head 4 sculpted [MASK] [UNK] [UNK] [UNK] was placed near the Constitution Avenue entrance of the Smithsonian [MASK] Museum of Natural History in [MASK] [MASK]. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "PREDICTIONS [CLS] law, D. The. A class of anp in the be in and [UNK] [UNK] was capture in the included American sale of the description species number of pre designed in the life. [SEP] in of,p of in - of were of were in - of of of in were pre in was in pre pre -\n",
      "ACTUAL      [CLS] Washington, D. C. A replica of San Lorenzo Head 4 sculpted by [UNK] [UNK] [UNK] was placed near the Constitution Avenue entrance of the Smithsonian National Museum of Natural History in October 2001. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
      "\n",
      "INPUTS      [CLS] After 30 years [MASK] [UNK] and a change in ownership [MASK] the station, his show was replaced with the [UNK] Network's The Larry King Show. In [MASK] 1980s, [UNK] [MASK] [MASK] at Buffalo's [UNK] College and served as [MASK] disc jockey on [MASK] Broadcasting'[MASK]zor station [UNK] [MASK] now [UNK] ). He also\n",
      "PREDICTIONS [CLS] It age â or [UNK] and a designed in television of the television, his life was designed with the Writers productions's \" Story or published. In Kenneth pieces by [UNK], designed ; track's [UNK] John and term as his by of in her life's life entitled [UNK] ; became [UNK] ). He became\n",
      "ACTUAL      [CLS] After 30 years with [UNK] and a change in ownership for the station, his show was replaced with the [UNK] Network's The Larry King Show. In the 1980s, [UNK] taught communications at Buffalo's [UNK] College and served as a disc jockey on Public Broadcasting's radio station [UNK] ( now [UNK] ). He also\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tiny_bert.eval()\n",
    "for masked_sentences, masks, unmasked_sentences in test_dataloader:    \n",
    "    masked_sentences = masked_sentences.to(device)\n",
    "    masks = masks.to(torch.bool)\n",
    "    masks = masks.to(device)\n",
    "    unmasked_sentences = unmasked_sentences.to(device)\n",
    "\n",
    "    word_logits = tiny_bert(masked_sentences.to(device))\n",
    "    print(word_logits.shape)\n",
    "    predictions = torch.argmax(word_logits, dim=-1)\n",
    "    for i in range(len(predictions)):\n",
    "        print(\"INPUTS     \", tokenizer.decode(masked_sentences[i]))\n",
    "        print(\"PREDICTIONS\", tokenizer.decode(predictions[i]))\n",
    "        print(\"ACTUAL     \", tokenizer.decode(unmasked_sentences[i]))\n",
    "        print()\n",
    "    break\n",
    " \n",
    "    # word_logits.shape: batch_size, seq_length, vocab_size\n",
    "#     # mask.shape = batch_size, seq_length\n",
    "#     # Mask the logits\n",
    "#     masked_logits = torch.masked_select(word_logits, masks.unsqueeze(-1))\n",
    "    \n",
    "#     # Reshaping to work with loss function\n",
    "#     masked_logits = einops.rearrange(masked_logits, '(num_masks vocab_size) -> num_masks vocab_size', \n",
    "#                                      vocab_size=vocab_size)\n",
    "#     target = torch.masked_select(unmasked_sentences, masks)\n",
    "\n",
    "#     # Averaging the loss across the sequences\n",
    "#     batch_size = masked_sentences.shape[0]\n",
    "#     loss = loss_fn(masked_logits, target) / batch_size\n",
    "        \n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "#     if gradient_steps % 20 == 0:\n",
    "#         print(gradient_steps, loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_list(data: list, max_seq_len: int, min_seq_len: int = 10):\n",
    "    \"\"\"Take in data_train or data_test, and output a dataloader (an iterable of sample x, label y)\"\"\"\n",
    "    # Reverse sample and label\n",
    "    # Tokenize\n",
    "    saved_data = list(data)\n",
    "    assert len(saved_data), \"No data left!\"\n",
    "    labels = [label for label, review in saved_data]\n",
    "    # reviews = [review for label, review in saved_data]\n",
    "    # tokenized_reviews = tokenizer.encode(reviews)\n",
    "    reviews = [review for label, review in saved_data]\n",
    "    tokenized_reviews = tokenizer(reviews)['input_ids']\n",
    "    \n",
    "    # Truncate based on max_seq_len\n",
    "    tokenized_reviews = [tokens[:max_seq_len] for tokens in tokenized_reviews]\n",
    "    \n",
    "    # Get padding token_id\n",
    "    # Pad to longest\n",
    "    longest_length = max([len(tokens) for tokens in tokenized_reviews])\n",
    "    tokenized_reviews = [tokens + [tokenizer.pad_token_id]*(longest_length-len(tokens)) for tokens in tokenized_reviews]\n",
    "    \n",
    "    # Shuffle\n",
    "    data_list = [(tokens, 0 if label == 'neg' else 1) for tokens, label in zip(tokenized_reviews, labels)]\n",
    "    random.shuffle(data_list)\n",
    "    return data_list\n",
    "    \n",
    "def get_batches(data, batch_size, max_seq_len):\n",
    "    shuffled_data_list = get_data_list(data, max_seq_len)\n",
    "    res = []\n",
    "    for batch_idx in range(math.ceil(len(shuffled_data_list) / batch_size)):\n",
    "        batch_data = shuffled_data_list[batch_idx*batch_size: (batch_idx+1)*batch_size]\n",
    "        reviews = [review for review, sentiment in batch_data]\n",
    "        sentiments = [sentiment for review, sentiment in batch_data]\n",
    "        res.append((torch.tensor(reviews, dtype=torch.long), torch.tensor(sentiments, dtype=torch.long)))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
