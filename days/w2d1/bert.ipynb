{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import torch.nn as nn\n",
    "from torch import einsum\n",
    "from einops import rearrange, reduce, repeat\n",
    "import bert_tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def raw_attention_pattern(token_activations, num_heads, project_query, project_key):\n",
    "    queries = project_query(token_activations)\n",
    "    keys = project_key(token_activations)\n",
    "    keys_reshaped = rearrange(keys, 'b l (h p) -> b h l p', h = num_heads)\n",
    "    queries_reshaped = rearrange(queries, 'b l (h p) -> b h l p', h = num_heads)\n",
    "    keys_times_queries = t.einsum('b h l p, b h m p -> b h l m', keys_reshaped, queries_reshaped) / t.sqrt(t.tensor(keys.shape[-1]//num_heads))\n",
    "    return keys_times_queries\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention pattern raw MATCH!!!!!!!!\n",
      " SHAPE (2, 12, 3, 3) MEAN: 0.01012 STD: 0.1032 VALS [-0.08612 0.01278 -0.009718 -0.2377 0.02676 0.1858 -0.05701 -0.1389 0.07155 -0.07107...]\n"
     ]
    }
   ],
   "source": [
    "bert_tests.test_attention_pattern_fn(raw_attention_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_attention(token_activations, num_heads, attention_pattern, project_value, project_output):\n",
    "    projected_input = project_value(token_activations)\n",
    "    soft_max = t.nn.functional.softmax(attention_pattern, dim=-2)\n",
    "    activations_reshaped = rearrange(projected_input, 'b l (h p) -> b h l p', h = num_heads)\n",
    "    weighted_activations = t.einsum('b h l m, b h l p -> b h m p', soft_max, activations_reshaped)\n",
    "    return project_output(rearrange(weighted_activations, 'b h m p -> b m (h p)'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "attention MATCH!!!!!!!!\n",
      " SHAPE (2, 3, 768) MEAN: 0.001565 STD: 0.1138 VALS [0.2294 0.06196 -0.05333 0.0651 -0.1487 0.02752 -0.02764 -0.07989 -0.252 -0.1724...]\n"
     ]
    }
   ],
   "source": [
    "bert_tests.test_attention_fn(bert_attention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedSelfAttention(nn.Module):\n",
    "    def __init__(self, num_heads, hidden_size):\n",
    "        super(MultiHeadedSelfAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.hidden_size = hidden_size\n",
    "        self.query = nn.Linear(hidden_size, hidden_size)\n",
    "        self.keys = nn.Linear(hidden_size, hidden_size)\n",
    "        self.project_value = nn.Linear(hidden_size, hidden_size)\n",
    "        self.project_out = nn.Linear(hidden_size, hidden_size)\n",
    "\n",
    "    \n",
    "    def forward(self, token_activations):\n",
    "        pattern = raw_attention_pattern(token_activations, self.num_heads, lambda a: self.query(a), lambda a: self.keys(a))\n",
    "        attn = bert_attention(token_activations, self.num_heads, pattern, lambda a: self.project_value(a), lambda a: self.project_out(a))\n",
    "        return attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert MATCH!!!!!!!!\n",
      " SHAPE (2, 3, 768) MEAN: -0.001554 STD: 0.1736 VALS [-0.08316 -0.09165 -0.03188 -0.03013 0.1001 0.09549 -0.1046 0.07742 0.0424 0.05553...]\n"
     ]
    }
   ],
   "source": [
    "bert_tests.test_bert_attention(MultiHeadedSelfAttention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_mlp(token_activations, linear_1, linear_2):\n",
    "    return linear_2(nn.functional.gelu(linear_1(token_activations)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert mlp MATCH!!!!!!!!\n",
      " SHAPE (2, 3, 768) MEAN: -0.0001934 STD: 0.1044 VALS [-0.1153 0.1189 -0.0813 0.1021 0.0296 0.06182 0.0341 0.1446 0.2622 -0.08507...]\n"
     ]
    }
   ],
   "source": [
    "bert_tests.test_bert_mlp(bert_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertMLP(nn.Module):\n",
    "    def __init__(self, input_size, intermediate_size):\n",
    "        super(BertMLP, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_size, intermediate_size)\n",
    "        self.linear2 = nn.Linear(intermediate_size, input_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return bert_mlp(x, self.linear1, self.linear2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, normalized_dim: int):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.weight = nn.Parameter(t.ones(normalized_dim))\n",
    "        self.bias = nn.Parameter(t.zeros(normalized_dim))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = (x - t.mean(x, dim=-1).detach().unsqueeze(-1))/t.std(x, dim = -1, unbiased=False).detach().unsqueeze(-1)\n",
    "        x = x * self.weight + self.bias\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layer norm MATCH!!!!!!!!\n",
      " SHAPE (20, 10) MEAN: -1.431e-08 STD: 1.003 VALS [0.6906 -0.84 1.881 1.711 -0.5117 -0.9577 -0.1387 -0.6943 -0.6741 -0.4662...]\n"
     ]
    }
   ],
   "source": [
    "bert_tests.test_layer_norm(LayerNorm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertBlock(nn.Module):\n",
    "    def __init__(self, hidden_size, intermediate_size, num_heads, dropout):\n",
    "        super(BertBlock, self).__init__()\n",
    "        self.attention = MultiHeadedSelfAttention(num_heads, hidden_size)\n",
    "        self.layer_norm = LayerNorm(hidden_size)\n",
    "        self.mlp = BertMLP(hidden_size, intermediate_size)\n",
    "        self.ln2 = LayerNorm(hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x_mhsa = self.attention(x)\n",
    "        x_ln1 = self.layer_norm(x_mhsa + x)\n",
    "        x_mlp = self.mlp(x_ln1)\n",
    "        x_dropout = self.dropout(x_mlp)\n",
    "        x_ln2 = self.ln2(x_dropout + x_ln1) \n",
    "        return x_ln2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert MATCH!!!!!!!!\n",
      " SHAPE (2, 3, 768) MEAN: -4.139e-09 STD: 1 VALS [0.007131 -0.04372 0.6502 -0.5972 -1.097 0.7267 0.1275 -0.6035 -0.2226 0.2145...]\n"
     ]
    }
   ],
   "source": [
    "bert_tests.test_bert_block(BertBlock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embed_size):\n",
    "        super(Embedding, self).__init__()\n",
    "        self.weight = nn.Parameter(t.randn(vocab_size, embed_size))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.weight[x.long(), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding MATCH!!!!!!!!\n",
      " SHAPE (2, 3, 5) MEAN: -0.06748 STD: 1.062 VALS [1.176 -0.1914 0.8212 1.047 -0.481 0.7106 -1.304 -1.307 -0.438 -0.2764...]\n"
     ]
    }
   ],
   "source": [
    "bert_tests.test_embedding(Embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_embedding(input_ids, token_type_ids, position_embedding, token_embedding, token_type_embedding, layer_norm, dropout):\n",
    "    device = \"cuda\" if input_ids.is_cuda else \"cpu\"\n",
    "    pos_emb = position_embedding(t.arange(0, input_ids.shape[1]).to(device))\n",
    "    tok_emb = token_embedding(input_ids)\n",
    "    typ_emb = token_type_embedding(token_type_ids)\n",
    "    emb = pos_emb + tok_emb + typ_emb\n",
    "    return dropout(layer_norm(emb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert embedding MATCH!!!!!!!!\n",
      " SHAPE (2, 3, 768) MEAN: 4.967e-09 STD: 1 VALS [-1.319 -0.4378 -2.074 0.9679 0.9274 1.479 -0.501 -1.9 -0.212 0.7961...]\n"
     ]
    }
   ],
   "source": [
    "bert_tests.test_bert_embedding_fn(bert_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, max_position_embeddings, type_vocab_size, dropout):\n",
    "        super(BertEmbedding, self).__init__()\n",
    "        self.token_embedding = Embedding(vocab_size, hidden_size)\n",
    "        self.position_embedding = Embedding(max_position_embeddings, hidden_size)\n",
    "        self.token_type_embedding = Embedding(type_vocab_size, hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = LayerNorm(hidden_size)\n",
    "    \n",
    "    def forward(self, input_ids, token_type_ids):\n",
    "        return bert_embedding(input_ids, token_type_ids, self.position_embedding, self.token_embedding, self.token_type_embedding, self.layer_norm, self.dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert embedding MATCH!!!!!!!!\n",
      " SHAPE (2, 3, 768) MEAN: -4.553e-09 STD: 1 VALS [-0.009385 -0.4919 0.9852 -0.3535 -3.624 1.333 1.163 1.449 1.063 0.246...]\n"
     ]
    }
   ],
   "source": [
    "bert_tests.test_bert_embedding(BertEmbedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bert(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, max_position_embeddings, type_vocab_size, dropout, intermediate_size, num_heads, num_layers):\n",
    "        super(Bert, self).__init__()\n",
    "        self.embedding = BertEmbedding(vocab_size, hidden_size, max_position_embeddings, type_vocab_size, dropout)\n",
    "        self.transformer = t.nn.Sequential(*[BertBlock(hidden_size, intermediate_size, num_heads, dropout) for _ in range(num_layers)])\n",
    "        self.mlp = nn.Linear(hidden_size, hidden_size)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.layer_norm = LayerNorm(hidden_size)\n",
    "        self.unembedding = nn.Linear(hidden_size, vocab_size)\n",
    "    \n",
    "    def forward(self, input_ids):\n",
    "        token_type_ids = t.zeros(input_ids.shape)\n",
    "        token_type_ids.to(\"cuda\" if input_ids.is_cuda else \"cpu\")\n",
    "        embedding = self.embedding(input_ids, token_type_ids)\n",
    "        output = self.transformer(embedding)\n",
    "        lin = self.mlp(output)\n",
    "        gelu = self.gelu(lin)\n",
    "        layernorm = self.layer_norm(gelu)\n",
    "        return self.unembedding(layernorm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert MATCH!!!!!!!!\n",
      " SHAPE (1, 4, 28996) MEAN: 0.003031 STD: 0.5765 VALS [-0.5742 -0.4321 0.1186 -0.7165 -0.5262 0.4967 1.223 0.3165 -0.3247 -0.5717...]\n"
     ]
    }
   ],
   "source": [
    "bert_tests.test_bert(Bert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "my_bert = Bert(\n",
    "    vocab_size=28996, hidden_size=768, max_position_embeddings=512, \n",
    "    type_vocab_size=2, dropout=0.1, intermediate_size=3072, \n",
    "    num_heads=12, num_layers=12\n",
    ")\n",
    "pretrained_bert = bert_tests.get_pretrained_bert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<All keys matched successfully>\n"
     ]
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "def repl(st):\n",
    "    st = st.replace(\"pattern.project_key\", \"keys\")\n",
    "    st = st.replace(\"pattern.project_query\", \"query\")\n",
    "    st = st.replace(\"residual.mlp\", \"mlp.linear\")\n",
    "    st = st.replace(\"residual.layer_norm\", \"ln2\")\n",
    "    st = st.replace(\"lm_head.\", \"\")\n",
    "    return st\n",
    "\n",
    "skip_params = [\"classification_head.weight\", \"classification_head.bias\"]\n",
    "    \n",
    "d = OrderedDict([(repl(k), v) for k,v in pretrained_bert.state_dict().items() if k not in skip_params])\n",
    "\n",
    "print(my_bert.load_state_dict(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "comparing Berts MATCH!!!!!!!!\n",
      " SHAPE (10, 20, 28996) MEAN: -2.732 STD: 2.414 VALS [-5.65 -6.041 -6.096 -6.062 -5.945 -5.777 -5.977 -6.015 -6.028 -5.935...]\n"
     ]
    }
   ],
   "source": [
    "bert_tests.test_same_output(my_bert, pretrained_bert, tol=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TOKENIZERS_PARALLELISM=false\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "\n",
    "%env TOKENIZERS_PARALLELISM=false\n",
    "tokenizer_uncased = transformers.AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[101, 1109, 1783, 18062, 8474, 1108, 4331, 3999, 103, 119, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
     ]
    }
   ],
   "source": [
    "inp = tokenizer([\"The firetruck was painted bright [MASK].\"])\n",
    "print(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ascii_art_probs(sentence, n_top, tkizer):\n",
    "    inp = tkizer([sentence])\n",
    "    probs = t.nn.functional.softmax(my_bert(t.Tensor(inp[\"input_ids\"])), dim=-1)\n",
    "    mask_num = tkizer([\"[MASK]\"])['input_ids'][0][1]\n",
    "    mask_idx = inp[\"input_ids\"][0].index(mask_num)\n",
    "    probs_mask = probs[:,mask_idx].squeeze()\n",
    "    sorted_probs = t.sort(probs_mask)\n",
    "    most_likely = sorted_probs.indices[-n_top:].flip(0)\n",
    "    probs = sorted_probs.values[-n_top:].flip(0)\n",
    "    print(sentence.replace(\"[MASK]\", \"______\"))\n",
    "    for i in range(n_top):\n",
    "        print(f\"Word: {tkizer.decode(most_likely[i])} \\t probability: {int(probs[i]*10000)/100}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The fish likes to eat ______\n",
      "Word: . \t probability: 94.29%\n",
      "Word: ; \t probability: 4.72%\n",
      "Word: ! \t probability: 0.89%\n",
      "Word: ? \t probability: 0.05%\n",
      "Word: ... \t probability: 0.0%\n",
      "Word: , \t probability: 0.0%\n",
      "Word: : \t probability: 0.0%\n",
      "Word: and \t probability: 0.0%\n",
      "Word: | \t probability: 0.0%\n",
      "Word: but \t probability: 0.0%\n"
     ]
    }
   ],
   "source": [
    "ascii_art_probs(\"The fish likes to eat [MASK]\", 10, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertClassifier(nn.Module):\n",
    "    def __init__(self,\n",
    "                 vocab_size,\n",
    "                 hidden_size,\n",
    "                 max_position_embeddings,\n",
    "                 type_vocab_size,\n",
    "                 dropout,\n",
    "                 intermediate_size,\n",
    "                 num_heads,\n",
    "                 num_layers,\n",
    "                 num_classes):\n",
    "        \n",
    "        super(BertClassifier, self).__init__()\n",
    "        self.embedding = BertEmbedding(vocab_size, hidden_size, max_position_embeddings, type_vocab_size, dropout)\n",
    "        self.transformer = t.nn.Sequential(*[BertBlock(hidden_size, intermediate_size, num_heads, dropout) for _ in range(num_layers)])\n",
    "        self.mlp = nn.Linear(hidden_size, hidden_size)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.layer_norm = LayerNorm(hidden_size)\n",
    "        self.unembedding = nn.Linear(hidden_size, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.classification_head = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, input_ids):\n",
    "        token_type_ids = t.zeros(input_ids.shape)\n",
    "        token_type_ids.to(\"cuda\" if input_ids.is_cuda else \"cpu\")\n",
    "        embedding = self.embedding(input_ids, token_type_ids)\n",
    "        output = self.transformer(embedding)\n",
    "        lin = self.mlp(output)\n",
    "        gelu = self.gelu(lin)\n",
    "        layernorm = self.layer_norm(gelu)\n",
    "        unembedding = self.unembedding(layernorm)\n",
    "        \n",
    "        dropout = self.dropout(output[:, 0, :])\n",
    "        classification = self.classification_head(dropout)\n",
    "        \n",
    "        return unembedding, classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<All keys matched successfully>\n"
     ]
    }
   ],
   "source": [
    "classifier = BertClassifier(vocab_size=28996, hidden_size=768, max_position_embeddings=512, \n",
    "    type_vocab_size=2, dropout=0.1, intermediate_size=3072, \n",
    "    num_heads=12, num_layers=12, num_classes=2)\n",
    "d = OrderedDict([(repl(k), v) for k,v in pretrained_bert.state_dict().items()])\n",
    "\n",
    "print(classifier.load_state_dict(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "data_train, data_test = torchtext.datasets.IMDB(root='.data', split=('train', 'test'))\n",
    "data = list(data_train).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import einops\n",
    "\n",
    "def preprocess(data, tokenizer, max_seq_len, batch_size):\n",
    "    all_data = []\n",
    "    labels = []\n",
    "    for label, text in data:\n",
    "        tokenized_text = tokenizer([text], padding='longest', max_length=max_seq_len, truncation=True)[\"input_ids\"][0]\n",
    "        if len(tokenized_text) < max_seq_len:\n",
    "            tokenized_text += [0] * (max_seq_len - len(tokenized_text))\n",
    "        # tokenized_text = tokenized_text[:max_seq_len]\n",
    "        all_data.append(tokenized_text)\n",
    "        labels.append(label)\n",
    "    all_data = t.Tensor(all_data[:len(all_data) - (len(all_data) % batch_size)])\n",
    "    labels = t.Tensor(list(map(lambda x: 0 if x == \"neg\" else 1, labels[:len(labels) - (len(labels) % batch_size)])))\n",
    "    perm = t.randperm(all_data.shape[0])\n",
    "    all_data = all_data[perm]\n",
    "    labels = labels[perm]\n",
    "    all_data = einops.rearrange(all_data, \"(k b) m -> k b m\", b = batch_size)\n",
    "    labels = einops.rearrange(labels, \"(k b) -> k b\", b = batch_size)\n",
    "    return all_data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1562, 16, 512]) torch.Size([1562, 16])\n"
     ]
    }
   ],
   "source": [
    "training_batches, training_labels = preprocess(data, tokenizer, 512, 16)\n",
    "print(training_batches.shape, training_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "batch 0 loss 2.6286661624908447\n",
      "batch 20 loss 0.7271848320960999\n",
      "batch 40 loss 0.7447864413261414\n",
      "batch 60 loss 0.631729006767273\n",
      "batch 80 loss 0.392570436000824\n",
      "batch 100 loss 0.7503160834312439\n",
      "batch 120 loss 0.44002634286880493\n",
      "batch 140 loss 0.1489362120628357\n",
      "batch 160 loss 0.30392324924468994\n",
      "batch 180 loss 0.28741395473480225\n",
      "batch 200 loss 0.21664977073669434\n",
      "batch 220 loss 0.6484529972076416\n",
      "batch 240 loss 0.2540418803691864\n",
      "batch 260 loss 0.5189374089241028\n",
      "batch 280 loss 0.0915040671825409\n",
      "batch 300 loss 0.15545439720153809\n",
      "batch 320 loss 0.405807763338089\n",
      "batch 340 loss 0.3349575996398926\n",
      "batch 360 loss 0.5228177309036255\n",
      "batch 380 loss 0.08513675630092621\n",
      "batch 400 loss 0.2544328570365906\n",
      "batch 420 loss 0.10949930548667908\n",
      "batch 440 loss 0.14662708342075348\n",
      "batch 460 loss 0.41021499037742615\n",
      "batch 480 loss 0.44425761699676514\n",
      "batch 500 loss 0.34592345356941223\n",
      "batch 520 loss 0.11588944494724274\n",
      "batch 540 loss 0.21250569820404053\n",
      "batch 560 loss 0.5014015436172485\n",
      "batch 580 loss 0.1076231598854065\n",
      "batch 600 loss 0.40630874037742615\n",
      "batch 620 loss 0.053342778235673904\n",
      "batch 640 loss 0.08653072267770767\n",
      "batch 660 loss 0.15588925778865814\n",
      "batch 680 loss 0.10588961094617844\n",
      "batch 700 loss 0.35825762152671814\n",
      "batch 720 loss 0.18877215683460236\n",
      "batch 740 loss 0.13874487578868866\n",
      "batch 760 loss 0.3001917004585266\n",
      "batch 780 loss 0.06329453736543655\n",
      "batch 800 loss 0.14199891686439514\n",
      "batch 820 loss 0.09193704277276993\n",
      "batch 840 loss 0.33127376437187195\n",
      "batch 860 loss 0.03736421838402748\n",
      "batch 880 loss 0.06411261111497879\n",
      "batch 900 loss 0.21037815511226654\n",
      "batch 920 loss 0.44954755902290344\n",
      "batch 940 loss 0.09591799229383469\n",
      "batch 960 loss 0.4224015474319458\n",
      "batch 980 loss 0.06220339238643646\n",
      "batch 1000 loss 0.19817791879177094\n",
      "batch 1020 loss 0.18702030181884766\n",
      "batch 1040 loss 0.15016627311706543\n",
      "batch 1060 loss 0.057863764464855194\n",
      "batch 1080 loss 0.09911595284938812\n",
      "batch 1100 loss 0.33088403940200806\n",
      "batch 1120 loss 0.14649732410907745\n",
      "batch 1140 loss 0.047229789197444916\n",
      "batch 1160 loss 0.6358790993690491\n",
      "batch 1180 loss 0.25435835123062134\n",
      "batch 1200 loss 0.21391797065734863\n",
      "batch 1220 loss 0.3510150909423828\n",
      "batch 1240 loss 0.2861764430999756\n",
      "batch 1260 loss 0.24122512340545654\n",
      "batch 1280 loss 0.16908949613571167\n",
      "batch 1300 loss 0.1541380137205124\n",
      "batch 1320 loss 0.06519153714179993\n",
      "batch 1340 loss 0.20599952340126038\n",
      "batch 1360 loss 0.06164985150098801\n",
      "batch 1380 loss 0.1056697815656662\n",
      "batch 1400 loss 0.20999348163604736\n",
      "batch 1420 loss 0.2138982117176056\n",
      "batch 1440 loss 0.05719146132469177\n",
      "batch 1460 loss 0.1497260183095932\n",
      "batch 1480 loss 0.2734575867652893\n",
      "batch 1500 loss 0.03406953811645508\n",
      "batch 1520 loss 0.2696199417114258\n",
      "batch 1540 loss 0.15442323684692383\n",
      "batch 1560 loss 0.14506052434444427\n",
      "epoch 1\n",
      "batch 0 loss 0.6918814778327942\n",
      "batch 20 loss 0.17355643212795258\n",
      "batch 40 loss 0.139181986451149\n",
      "batch 60 loss 0.0680188238620758\n",
      "batch 80 loss 0.01685236766934395\n",
      "batch 100 loss 0.33339807391166687\n",
      "batch 120 loss 0.18693533539772034\n",
      "batch 140 loss 0.03925172612071037\n",
      "batch 160 loss 0.0625244602560997\n",
      "batch 180 loss 0.07947912812232971\n",
      "batch 200 loss 0.035835303366184235\n",
      "batch 220 loss 0.8490650057792664\n",
      "batch 240 loss 0.3860284388065338\n",
      "batch 260 loss 0.4075499475002289\n",
      "batch 280 loss 0.06849861890077591\n",
      "batch 300 loss 0.0246882326900959\n",
      "batch 320 loss 0.23524460196495056\n",
      "batch 340 loss 0.07304323464632034\n",
      "batch 360 loss 0.45064955949783325\n",
      "batch 380 loss 0.16173070669174194\n",
      "batch 400 loss 0.23100028932094574\n",
      "batch 420 loss 0.025154922157526016\n",
      "batch 440 loss 0.12005966156721115\n",
      "batch 460 loss 0.13342054188251495\n",
      "batch 480 loss 0.21399669349193573\n",
      "batch 500 loss 0.07164368033409119\n",
      "batch 520 loss 0.03523553907871246\n",
      "batch 540 loss 0.09788702428340912\n",
      "batch 560 loss 0.603261411190033\n",
      "batch 580 loss 0.0883731096982956\n",
      "batch 600 loss 0.30965155363082886\n",
      "batch 620 loss 0.18977664411067963\n",
      "batch 640 loss 0.0222328994423151\n",
      "batch 660 loss 0.16419588029384613\n",
      "batch 680 loss 0.11054572463035583\n",
      "batch 700 loss 0.1356247216463089\n",
      "batch 720 loss 0.019923970103263855\n",
      "batch 740 loss 0.19096072018146515\n",
      "batch 760 loss 0.16444697976112366\n",
      "batch 780 loss 0.05255918577313423\n",
      "batch 800 loss 0.035777900367975235\n",
      "batch 820 loss 0.017254237085580826\n",
      "batch 840 loss 0.04462115839123726\n",
      "batch 860 loss 0.01668836735188961\n",
      "batch 880 loss 0.008623586967587471\n",
      "batch 900 loss 0.2933541536331177\n",
      "batch 920 loss 0.6807695627212524\n",
      "batch 940 loss 0.08646444231271744\n",
      "batch 960 loss 0.37136104702949524\n",
      "batch 980 loss 0.11991063505411148\n",
      "batch 1000 loss 0.032548174262046814\n",
      "batch 1020 loss 0.15325424075126648\n",
      "batch 1040 loss 0.02941478043794632\n",
      "batch 1060 loss 0.07292475551366806\n",
      "batch 1080 loss 0.012439200654625893\n",
      "batch 1100 loss 0.29489120841026306\n",
      "batch 1120 loss 0.3844914734363556\n",
      "batch 1140 loss 0.02289108745753765\n",
      "batch 1160 loss 0.44267648458480835\n",
      "batch 1180 loss 0.07811388373374939\n",
      "batch 1200 loss 0.040730997920036316\n",
      "batch 1220 loss 0.3733168840408325\n",
      "batch 1240 loss 0.0869622454047203\n",
      "batch 1260 loss 0.07598353177309036\n",
      "batch 1280 loss 0.10732745379209518\n",
      "batch 1300 loss 0.1333438903093338\n",
      "batch 1320 loss 0.012324506416916847\n",
      "batch 1340 loss 0.17643393576145172\n",
      "batch 1360 loss 0.02637319825589657\n",
      "batch 1380 loss 0.0379537008702755\n",
      "batch 1400 loss 0.0147054772824049\n",
      "batch 1420 loss 0.08352303504943848\n",
      "batch 1440 loss 0.036872636526823044\n",
      "batch 1460 loss 0.020031170919537544\n",
      "batch 1480 loss 0.13798564672470093\n",
      "batch 1500 loss 0.01593169756233692\n",
      "batch 1520 loss 0.17972315847873688\n",
      "batch 1540 loss 0.010452073998749256\n",
      "batch 1560 loss 0.16197682917118073\n",
      "epoch 2\n",
      "batch 0 loss 0.23532041907310486\n",
      "batch 20 loss 0.08409764617681503\n",
      "batch 40 loss 0.04144690930843353\n",
      "batch 60 loss 0.019279232248663902\n",
      "batch 80 loss 0.002719233511015773\n",
      "batch 100 loss 0.11651964485645294\n",
      "batch 120 loss 0.07573338598012924\n",
      "batch 140 loss 0.01121988333761692\n",
      "batch 160 loss 0.01638343557715416\n",
      "batch 180 loss 0.14962898194789886\n",
      "batch 200 loss 0.03999636694788933\n",
      "batch 220 loss 0.4823993146419525\n",
      "batch 240 loss 0.14635728299617767\n",
      "batch 260 loss 0.4868927299976349\n",
      "batch 280 loss 0.030798539519309998\n",
      "batch 300 loss 0.0042036110535264015\n",
      "batch 320 loss 0.03729715943336487\n",
      "batch 340 loss 0.051141802221536636\n",
      "batch 360 loss 0.2969861924648285\n",
      "batch 380 loss 0.22887299954891205\n",
      "batch 400 loss 0.032721564173698425\n",
      "batch 420 loss 0.011694420129060745\n",
      "batch 440 loss 0.3259895145893097\n",
      "batch 460 loss 0.0449911430478096\n",
      "batch 480 loss 0.23515929281711578\n",
      "batch 500 loss 0.058092210441827774\n",
      "batch 520 loss 0.012405090034008026\n",
      "batch 540 loss 0.053962692618370056\n",
      "batch 560 loss 0.4756236672401428\n",
      "batch 580 loss 0.011350629851222038\n",
      "batch 600 loss 0.2045501470565796\n",
      "batch 620 loss 0.0030355544295161963\n",
      "batch 640 loss 0.011330952867865562\n",
      "batch 660 loss 0.2043655812740326\n",
      "batch 680 loss 0.007005838677287102\n",
      "batch 700 loss 0.30609098076820374\n",
      "batch 720 loss 0.019329017028212547\n",
      "batch 740 loss 0.07486103475093842\n",
      "batch 760 loss 0.00386519986204803\n",
      "batch 780 loss 0.009827067144215107\n",
      "batch 800 loss 0.028028661385178566\n",
      "batch 820 loss 0.04798484593629837\n",
      "batch 840 loss 0.06473911553621292\n",
      "batch 860 loss 0.003466985421255231\n",
      "batch 880 loss 0.006217285059392452\n",
      "batch 900 loss 0.20279328525066376\n",
      "batch 920 loss 0.20893192291259766\n",
      "batch 940 loss 0.010528016835451126\n",
      "batch 960 loss 0.41735851764678955\n",
      "batch 980 loss 0.0032259051222354174\n",
      "batch 1000 loss 0.2125885784626007\n",
      "batch 1020 loss 0.0076369293965399265\n",
      "batch 1040 loss 0.05389241874217987\n",
      "batch 1060 loss 0.008175812661647797\n",
      "batch 1080 loss 0.0048835561610758305\n",
      "batch 1100 loss 0.021547015756368637\n",
      "batch 1120 loss 0.0092464005574584\n",
      "batch 1140 loss 0.13287793099880219\n",
      "batch 1160 loss 0.2885890603065491\n",
      "batch 1180 loss 0.3230273127555847\n",
      "batch 1200 loss 0.011917853727936745\n",
      "batch 1220 loss 0.2595602571964264\n",
      "batch 1240 loss 0.04969006031751633\n",
      "batch 1260 loss 0.05058858171105385\n",
      "batch 1280 loss 0.3670736253261566\n",
      "batch 1300 loss 0.18580307066440582\n",
      "batch 1320 loss 0.003165082074701786\n",
      "batch 1340 loss 0.05134331062436104\n",
      "batch 1360 loss 0.015699828043580055\n",
      "batch 1380 loss 0.3620985150337219\n",
      "batch 1400 loss 0.0635138526558876\n",
      "batch 1420 loss 0.08030494302511215\n",
      "batch 1440 loss 0.009522411040961742\n",
      "batch 1460 loss 0.006946936249732971\n",
      "batch 1480 loss 0.09392811357975006\n",
      "batch 1500 loss 0.03805026412010193\n",
      "batch 1520 loss 0.010205271653831005\n",
      "batch 1540 loss 0.0029874141328036785\n",
      "batch 1560 loss 0.07609748095273972\n"
     ]
    }
   ],
   "source": [
    "adam = t.optim.Adam(classifier.parameters(), 1e-5)\n",
    "classifier.train()\n",
    "classifier.cuda()\n",
    "t.cuda.empty_cache()\n",
    "num_batches = training_batches.shape[0]\n",
    "batch_size = training_batches.shape[1]\n",
    "for epoch in range(3):\n",
    "    print(\"epoch\", epoch)\n",
    "    for batch_num in range(num_batches):\n",
    "        adam.zero_grad()\n",
    "        b = training_batches[batch_num].cuda()\n",
    "        l = training_labels[batch_num].cuda()\n",
    "        out = classifier(b)[1]\n",
    "        out_loss = nn.functional.cross_entropy(out, l.long())\n",
    "        out_loss.backward()\n",
    "        adam.step()\n",
    "        if batch_num % 20 == 0:\n",
    "            print(\"batch\", batch_num, \"loss\", out_loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = list(data_test).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.save(classifier.state_dict(), \"classifier.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier2 =  BertClassifier(vocab_size=28996, hidden_size=768, max_position_embeddings=512, \n",
    "    type_vocab_size=2, dropout=0.1, intermediate_size=3072, \n",
    "    num_heads=12, num_layers=12, num_classes=2)\n",
    "classifier2.load_state_dict(t.load(\"classifier.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [[101, 146, 1108, 6764, 1105, 4853, 1107, 170, 1363, 1236, 119, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.3176, 0.6824]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer([\"I was shocked and confused in a good way.\"])\n",
    "print(tokens)\n",
    "t.softmax(classifier2(t.Tensor(tokens[\"input_ids\"]))[1], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1562, 16, 512]) torch.Size([1562, 16])\n"
     ]
    }
   ],
   "source": [
    "test_batches, test_labels = preprocess(test_data, tokenizer, 512, 16)\n",
    "print(test_batches.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig_classifier = BertClassifier(vocab_size=28996, hidden_size=768, max_position_embeddings=512, \n",
    "    type_vocab_size=2, dropout=0.1, intermediate_size=3072, \n",
    "    num_heads=12, num_layers=12, num_classes=2)\n",
    "d = OrderedDict([(repl(k), v) for k,v in pretrained_bert.state_dict().items()])\n",
    "orig_classifier.load_state_dict(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our losses: 2.4411317789927125, untrained losses: 6.9597057700157166\n",
      "Our losses: 4.395634302869439, untrained losses: 14.038484454154968\n",
      "Our losses: 6.618245888967067, untrained losses: 21.090316772460938\n",
      "Our losses: 8.400710095185786, untrained losses: 28.131726503372192\n",
      "Our losses: 11.236966487485915, untrained losses: 35.11941158771515\n",
      "Our losses: 13.49742683628574, untrained losses: 42.1220446228981\n",
      "Our losses: 14.890941345598549, untrained losses: 49.151991188526154\n",
      "Our losses: 17.449096416588873, untrained losses: 56.359111964702606\n",
      "Our losses: 19.597863247152418, untrained losses: 63.2793892621994\n",
      "Our losses: 22.138968688901514, untrained losses: 70.4475582242012\n",
      "Our losses: 24.37586442474276, untrained losses: 77.4146329164505\n",
      "Our losses: 26.547236795537174, untrained losses: 84.50524258613586\n",
      "Our losses: 32.004129777662456, untrained losses: 91.6532335281372\n",
      "Our losses: 35.05536588234827, untrained losses: 98.58932375907898\n",
      "Our losses: 37.947907514404505, untrained losses: 105.65718883275986\n",
      "Our losses: 40.69472038699314, untrained losses: 112.45124173164368\n",
      "Our losses: 43.362454624380916, untrained losses: 119.57244908809662\n",
      "Our losses: 45.91178309870884, untrained losses: 126.43935114145279\n",
      "Our losses: 48.229296987410635, untrained losses: 133.45025956630707\n",
      "Our losses: 50.56252234033309, untrained losses: 140.4750850200653\n",
      "Our losses: 52.52797083067708, untrained losses: 147.51238524913788\n",
      "Our losses: 55.77325450046919, untrained losses: 154.3669119477272\n",
      "Our losses: 57.729876479832456, untrained losses: 161.5279821753502\n",
      "Our losses: 60.16209635022096, untrained losses: 168.74184715747833\n",
      "Our losses: 63.04271321441047, untrained losses: 175.87796527147293\n",
      "Our losses: 64.63471844117157, untrained losses: 182.62118965387344\n",
      "Our losses: 67.42686274717562, untrained losses: 189.80427014827728\n",
      "Our losses: 69.55721976864152, untrained losses: 196.7604523897171\n",
      "Our losses: 71.09241627599113, untrained losses: 203.7630495429039\n",
      "Our losses: 73.12692140205763, untrained losses: 210.7678411602974\n",
      "Our losses: 75.60432468983345, untrained losses: 217.70723116397858\n",
      "Our losses: 78.5374269343447, untrained losses: 224.6962856054306\n",
      "Our losses: 81.43180635082535, untrained losses: 231.67894381284714\n",
      "Our losses: 83.95384588534944, untrained losses: 238.6213703751564\n",
      "Our losses: 86.6826169535052, untrained losses: 245.7608128786087\n",
      "Our losses: 88.3667502279859, untrained losses: 252.84054100513458\n",
      "Our losses: 91.19332842412405, untrained losses: 259.94698864221573\n",
      "Our losses: 94.0353524077218, untrained losses: 267.0107737183571\n",
      "Our losses: 98.18752940627746, untrained losses: 273.99584114551544\n",
      "Our losses: 101.61879416438751, untrained losses: 280.9446545243263\n",
      "Our losses: 102.9463249866385, untrained losses: 287.92419266700745\n",
      "Our losses: 104.45194759895094, untrained losses: 294.9675443768501\n",
      "Our losses: 106.87971228756942, untrained losses: 301.94965267181396\n",
      "Our losses: 108.4709260633681, untrained losses: 308.7305785417557\n",
      "Our losses: 111.98822109610774, untrained losses: 315.687417447567\n",
      "Our losses: 114.30229915981181, untrained losses: 322.68627494573593\n",
      "Our losses: 117.4692188159097, untrained losses: 329.80058574676514\n",
      "Our losses: 120.15811125584878, untrained losses: 336.82634150981903\n",
      "Our losses: 123.06651303567924, untrained losses: 343.7174137234688\n",
      "Our losses: 125.54848562902771, untrained losses: 350.6926317214966\n",
      "Our losses: 128.09216851531528, untrained losses: 357.66770803928375\n",
      "Our losses: 129.7616101980675, untrained losses: 364.5720998644829\n",
      "Our losses: 134.3223400034476, untrained losses: 371.65152275562286\n",
      "Our losses: 136.11183958151378, untrained losses: 378.723539352417\n",
      "Our losses: 138.73301114304923, untrained losses: 385.70641255378723\n",
      "Our losses: 141.32630271441303, untrained losses: 392.86685848236084\n",
      "Our losses: 143.8005286490079, untrained losses: 399.8186232447624\n",
      "Our losses: 147.0328200792428, untrained losses: 406.96485060453415\n",
      "Our losses: 149.22995016514324, untrained losses: 413.9798504114151\n",
      "Our losses: 151.23915052763186, untrained losses: 420.8843159675598\n",
      "Our losses: 153.9572826025542, untrained losses: 427.98710280656815\n",
      "Our losses: 156.70281716040336, untrained losses: 435.1733009815216\n",
      "Our losses: 158.8068100672681, untrained losses: 442.2965980768204\n",
      "Our losses: 160.60971340094693, untrained losses: 449.3544030189514\n",
      "Our losses: 163.80918244482018, untrained losses: 456.45976316928864\n",
      "Our losses: 166.46805906365626, untrained losses: 463.32120752334595\n",
      "Our losses: 168.4385824247729, untrained losses: 470.2325472831726\n",
      "Our losses: 173.08476243237965, untrained losses: 477.1183731555939\n",
      "Our losses: 176.50085872109048, untrained losses: 484.03622990846634\n",
      "Our losses: 180.35888966242783, untrained losses: 490.9370833039284\n",
      "Our losses: 183.15465611289255, untrained losses: 498.0106261372566\n",
      "Our losses: 185.66793574183248, untrained losses: 504.99272936582565\n",
      "Our losses: 188.22423520567827, untrained losses: 511.87182211875916\n",
      "Our losses: 190.0786183483433, untrained losses: 518.8627425432205\n",
      "Our losses: 194.1444971121382, untrained losses: 525.5506626963615\n",
      "Our losses: 195.80009794281796, untrained losses: 532.5194544196129\n",
      "Our losses: 198.16203134739771, untrained losses: 539.4200947284698\n",
      "Our losses: 200.56666180118918, untrained losses: 546.190855383873\n",
      "Our losses: 203.97797052003443, untrained losses: 553.2186979651451\n",
      "Our losses: 205.77569236978889, untrained losses: 560.3200115561485\n",
      "Our losses: 207.47827758267522, untrained losses: 567.3574170470238\n",
      "Our losses: 211.26505457703024, untrained losses: 574.3088564872742\n",
      "Our losses: 212.9397180043161, untrained losses: 581.3649141788483\n",
      "Our losses: 215.2436710484326, untrained losses: 588.437660574913\n",
      "Our losses: 217.98374872654676, untrained losses: 595.361866414547\n",
      "Our losses: 220.68366432003677, untrained losses: 602.3364961147308\n",
      "Our losses: 223.5013639125973, untrained losses: 609.4005189538002\n",
      "Our losses: 225.04398253839463, untrained losses: 616.4521791338921\n",
      "Our losses: 228.72869353089482, untrained losses: 623.2843134403229\n",
      "Our losses: 229.6583889015019, untrained losses: 630.1659258008003\n",
      "Our losses: 232.3012901567854, untrained losses: 637.0832590460777\n",
      "Our losses: 233.82941239560023, untrained losses: 644.0297739505768\n",
      "Our losses: 237.29424886452034, untrained losses: 650.919590651989\n",
      "Our losses: 239.32607535412535, untrained losses: 657.8918343782425\n",
      "Our losses: 241.44004940753803, untrained losses: 664.9416392445564\n",
      "Our losses: 243.87871656706557, untrained losses: 671.9355998635292\n",
      "Our losses: 245.7546831476502, untrained losses: 678.8260610103607\n",
      "Our losses: 247.8239128938876, untrained losses: 685.6206821203232\n",
      "Our losses: 252.30195931112394, untrained losses: 692.7257469296455\n",
      "Our losses: 256.6247501424514, untrained losses: 699.7804977893829\n",
      "Our losses: 259.03509463137016, untrained losses: 706.8101742267609\n",
      "Our losses: 260.2358363782987, untrained losses: 713.8329948782921\n",
      "Our losses: 263.47378172399476, untrained losses: 720.8452749848366\n",
      "Our losses: 267.2363863871433, untrained losses: 727.8317802548409\n",
      "Our losses: 270.72977086668834, untrained losses: 734.764141201973\n",
      "Our losses: 273.9626665599644, untrained losses: 741.5627537965775\n",
      "Our losses: 275.8588135037571, untrained losses: 748.6040895581245\n",
      "Our losses: 278.48824028484523, untrained losses: 755.7148109674454\n",
      "Our losses: 280.88868494331837, untrained losses: 762.6325438022614\n",
      "Our losses: 283.31319550517946, untrained losses: 769.8335587382317\n",
      "Our losses: 286.54961382132024, untrained losses: 776.6116931438446\n",
      "Our losses: 288.4541876488365, untrained losses: 783.6343222856522\n",
      "Our losses: 291.7155723744072, untrained losses: 790.7391626834869\n",
      "Our losses: 293.9821136505343, untrained losses: 797.7160267829895\n",
      "Our losses: 296.77515668654814, untrained losses: 804.7483329176903\n",
      "Our losses: 299.2659534350969, untrained losses: 811.7826465964317\n",
      "Our losses: 301.39070198265836, untrained losses: 818.6773860454559\n",
      "Our losses: 303.860644669272, untrained losses: 825.8390597701073\n",
      "Our losses: 305.64701810479164, untrained losses: 832.8358018994331\n",
      "Our losses: 308.6158061027527, untrained losses: 839.7855620384216\n",
      "Our losses: 310.7754069631919, untrained losses: 846.9696766734123\n",
      "Our losses: 315.3475975943729, untrained losses: 853.9158760905266\n",
      "Our losses: 318.1071367813274, untrained losses: 860.8043056726456\n",
      "Our losses: 321.2260444294661, untrained losses: 867.7109116315842\n",
      "Our losses: 323.9411860629916, untrained losses: 874.6552450656891\n",
      "Our losses: 326.20527636259794, untrained losses: 881.4476581215858\n",
      "Our losses: 330.0247974656522, untrained losses: 888.5999170541763\n",
      "Our losses: 332.9823468569666, untrained losses: 895.6516072154045\n",
      "Our losses: 337.0285008419305, untrained losses: 902.464818239212\n",
      "Our losses: 339.35051674954593, untrained losses: 909.4479296803474\n",
      "Our losses: 340.6286876555532, untrained losses: 916.5671246647835\n",
      "Our losses: 342.5242778575048, untrained losses: 923.8120480775833\n",
      "Our losses: 345.72544197179377, untrained losses: 930.9352630376816\n",
      "Our losses: 348.2848663292825, untrained losses: 937.9380186796188\n",
      "Our losses: 350.68952201027423, untrained losses: 944.7272805571556\n",
      "Our losses: 352.8092832127586, untrained losses: 951.6852744221687\n",
      "Our losses: 356.14109079167247, untrained losses: 958.7142643332481\n",
      "Our losses: 359.3345767483115, untrained losses: 965.7002803087234\n",
      "Our losses: 362.1106173880398, untrained losses: 972.8150400519371\n",
      "Our losses: 365.0419727638364, untrained losses: 979.7835334539413\n",
      "Our losses: 366.8411924559623, untrained losses: 986.7200720906258\n",
      "Our losses: 368.9635856691748, untrained losses: 993.745150744915\n",
      "Our losses: 371.70000819303095, untrained losses: 1001.0349391102791\n",
      "Our losses: 374.9383859504014, untrained losses: 1007.9783936738968\n",
      "Our losses: 377.16252736561, untrained losses: 1015.1019694805145\n",
      "Our losses: 379.70519689284265, untrained losses: 1022.0290015935898\n",
      "Our losses: 382.0757613237947, untrained losses: 1028.9106031656265\n",
      "Our losses: 384.4381385128945, untrained losses: 1035.7903693318367\n",
      "Our losses: 387.30640732776374, untrained losses: 1042.6492313742638\n",
      "Our losses: 390.28583919163793, untrained losses: 1049.629880130291\n",
      "Our losses: 393.19361884798855, untrained losses: 1056.5973827838898\n",
      "Our losses: 394.78046159259975, untrained losses: 1063.6622768044472\n",
      "Our losses: 396.6891415156424, untrained losses: 1070.6490307450294\n",
      "Our losses: 399.4146681949496, untrained losses: 1077.6076356768608\n",
      "Our losses: 401.0200593932532, untrained losses: 1084.6253935098648\n",
      "Our losses: 404.0149215622805, untrained losses: 1091.6488919258118\n"
     ]
    }
   ],
   "source": [
    "t.cuda.empty_cache()\n",
    "orig_classifier.cuda()\n",
    "test_loss = nn.CrossEntropyLoss()\n",
    "classifier.eval()\n",
    "orig_classifier.eval()\n",
    "\n",
    "our_losses = 0\n",
    "untrained_losses = 0\n",
    "\n",
    "example = test_batches[5].cuda()\n",
    "label = test_labels[5].long().cuda()\n",
    "\n",
    "for i in range(test_batches.shape[0]):\n",
    "    example = test_batches[i].cuda()\n",
    "    label = test_labels[i].long().cuda()\n",
    "\n",
    "    with t.no_grad():\n",
    "        out = classifier(example)[1]\n",
    "        out_orig = orig_classifier(example)[1]\n",
    "        our_losses += test_loss(out, label).item()\n",
    "        untrained_losses += test_loss(out_orig, label).item()\n",
    "        if i % 10 == 9:\n",
    "            print(f\"Our losses: {our_losses}, untrained losses: {untrained_losses}\")\n",
    "        #print(test_loss(out, label))\n",
    "        #print(test_loss(out_orig, label))\n",
    "    # print(out, out_orig, label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "[' \\n', ' = Valkyria Chronicles III = \\n', ' \\n', ' Senj no Valkyria 3 : <unk> Chronicles ( Japanese : 3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles III outside Japan , is a tactical role @-@ playing video game developed by Sega and Media.Vision for the PlayStation Portable . Released in January 2011 in Japan , it is the third game in the Valkyria series . <unk> the same fusion of tactical and real @-@ time gameplay as its predecessors , the story runs parallel to the first game and follows the \" Nameless \" , a penal military unit serving the nation of Gallia during the Second Europan War who perform secret black operations and are pitted against the Imperial unit \" <unk> Raven \" . \\n', \" The game began development in 2010 , carrying over a large portion of the work done on Valkyria Chronicles II . While it retained the standard features of the series , it also underwent multiple adjustments , such as making the game more <unk> for series newcomers . Character designer <unk> Honjou and composer Hitoshi Sakimoto both returned from previous entries , along with Valkyria Chronicles II director Takeshi Ozawa . A large team of writers handled the script . The game 's opening theme was sung by May 'n . \\n\", \" It met with positive sales in Japan , and was praised by both Japanese and western critics . After release , it received downloadable content , along with an expanded edition in November of that year . It was also adapted into manga and an original video animation series . Due to low sales of Valkyria Chronicles II , Valkyria Chronicles III was not localized , but a fan translation compatible with the game 's expanded edition was released in 2014 . Media.Vision would return to the franchise with the development of Valkyria : Azure Revolution for the PlayStation 4 . \\n\", ' \\n', ' = = Gameplay = = \\n', ' \\n', \" As with previous <unk> Chronicles games , Valkyria Chronicles III is a tactical role @-@ playing game where players take control of a military unit and take part in missions against enemy forces . Stories are told through comic book @-@ like panels with animated character portraits , with characters speaking partially through voiced speech bubbles and partially through <unk> text . The player progresses through a series of linear missions , gradually unlocked as maps that can be freely <unk> through and replayed as they are unlocked . The route to each story location on the map varies depending on an individual player 's approach : when one option is selected , the other is sealed off to the player . Outside missions , the player characters rest in a camp , where units can be customized and character growth occurs . Alongside the main story missions are character @-@ specific sub missions relating to different squad members . After the game 's completion , additional episodes are unlocked , some of them having a higher difficulty than those found in the rest of the game . There are also love simulation elements related to the game 's two main <unk> , although they take a very minor role . \\n\"]\n"
     ]
    }
   ],
   "source": [
    "from torchtext.datasets import WikiText2\n",
    "data_train, data_test = WikiText2(root='.data', split=('train', 'test'))\n",
    "wiki_train = list(data_train)[:1000].copy()\n",
    "wiki_test = list(data_test).copy()\n",
    "print(len(wiki_train))\n",
    "print(wiki_train[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(wiki_train[57])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def mask_wiki(data, tokenizer, max_seq_len):\n",
    "    orig_data = []\n",
    "    all_data = []\n",
    "    lengths = []\n",
    "    for text in data:\n",
    "        tokenized_text = tokenizer([text], max_length=max_seq_len, truncation=True)[\"input_ids\"][0]\n",
    "        lengths.append(len(tokenized_text))\n",
    "        all_data += tokenized_text\n",
    "        orig_data.append(tokenized_text)\n",
    "        # if len(tokenized_text) < max_seq_len:\n",
    "        #     tokenized_text += [0] * (max_seq_len - len(tokenized_text))\n",
    "    for i in range(len(all_data)):\n",
    "        if random.random() < 0.15:\n",
    "            if random.random() < 0.8:\n",
    "                all_data[i] = 103 # [MASK] token\n",
    "            elif random.random() < 0.5:\n",
    "                idx = random.randint(0, len(all_data))\n",
    "                all_data[i] = all_data[idx]\n",
    "    reshaped_data = []\n",
    "    idx = 0\n",
    "    for i in range(len(lengths)):\n",
    "        reshaped_data.append(all_data[idx:idx + lengths[i]])\n",
    "        idx += lengths[i]\n",
    "    return reshaped_data, orig_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Senj no Valkyria 3 : <unk> Chronicles ( Japanese : 3 , lit . Valkyria of the Battlefield 3 ) , commonly referred to as Valkyria Chronicles III outside Japan , is a tactical role @-@ playing video game developed by Sega and Media.Vision for the PlayStation Portable . Released in January 2011 in Japan , it is the third game in the Valkyria series . <unk> the same fusion of tactical and real @-@ time gameplay as its predecessors , the story runs parallel to the first game and follows the \" Nameless \" , a penal military unit serving the nation of Gallia during the Second Europan War who perform secret black operations and are pitted against the Imperial unit \" <unk> Raven \" . \n",
      "\n",
      "torch.Size([1000, 512])\n"
     ]
    }
   ],
   "source": [
    "import torch as t\n",
    "print(wiki_train[3])\n",
    "max_seq_len = 512\n",
    "masked_wiki, wiki = mask_wiki(wiki_train, tokenizer, max_seq_len)\n",
    "for i in range(len(masked_wiki)):\n",
    "    if len(masked_wiki[i]) < max_seq_len:\n",
    "        masked_wiki[i] += [0] * (max_seq_len - len(masked_wiki[i]))\n",
    "        wiki[i] += [0] * (max_seq_len - len(wiki[i]))\n",
    "masked_wiki = t.Tensor(masked_wiki)\n",
    "wiki = t.Tensor(wiki)\n",
    "print(masked_wiki.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_mask_token = (masked_wiki == 103)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 28996\n",
    "tiny_bert = Bert(\n",
    "    vocab_size=vocab_size, hidden_size=384, max_position_embeddings=512, \n",
    "    type_vocab_size=2, dropout=0.1, intermediate_size=1536, \n",
    "    num_heads=12, num_layers=2\n",
    ")\n",
    "tiny_bert.eval()\n",
    "probs = tiny_bert(masked_wiki)\n",
    "masked_tokens = wiki.masked_select(is_mask_token)\n",
    "print(masked_tokens)\n",
    "print(probs.shape)\n",
    "predictions = einops.rearrange(probs.masked_select(is_mask_token.unsqueeze(-1)), \"(h w) -> h w\", w= vocab_size)\n",
    "actual = t.zeros_like(predictions)\n",
    "for i in range(masked_tokens.shape[0]):\n",
    "    actual[i, masked_tokens[i].int()] = 1\n",
    "    \n",
    "print(predictions.shape, actual.shape)\n",
    "    \n",
    "with t.no_grad():\n",
    "    print(predictions[[0]], actual[[0]])\n",
    "    print(predictions[[1]], actual[[1]])\n",
    "    print(loss(predictions[[0]], actual[[0]]))\n",
    "    print(loss(predictions[[1]], actual[[1]]))\n",
    "    print(loss(predictions, actual))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_labels(wiki, masked_wiki, vocab_size):\n",
    "    is_mask_token = (masked_wiki == 103)\n",
    "    masked_tokens = wiki.masked_select(is_mask_token)\n",
    "    actual = t.zeros(masked_tokens.shape[0], vocab_size)\n",
    "    print(wiki.shape)\n",
    "    print(actual.shape)\n",
    "    for i in range(masked_tokens.shape[0]):\n",
    "        actual[i, masked_tokens[i].int()] = 1\n",
    "    return actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5\n",
    "batched_masked_wiki = einops.rearrange(masked_wiki[masked_wiki.shape[0]%batch_size:], \"(k b) l -> k b l\", b = batch_size)\n",
    "batched_wiki = einops.rearrange(wiki[wiki.shape[0]%batch_size:], \"(k b) l -> k b l\", b = batch_size)\n",
    "print(batched_masked_wiki.shape, batched_wiki.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = nn.CrossEntropyLoss()\n",
    "tiny_bert = Bert(\n",
    "    vocab_size=vocab_size, hidden_size=384, max_position_embeddings=512, \n",
    "    type_vocab_size=2, dropout=0.1, intermediate_size=1536, \n",
    "    num_heads=12, num_layers=2\n",
    ")\n",
    "adam = t.optim.Adam(tiny_bert.parameters(), 1e-5)\n",
    "tiny_bert.train()\n",
    "tiny_bert.cuda()\n",
    "t.cuda.empty_cache()\n",
    "num_batches = batched_masked_wiki.shape[0]\n",
    "batch_size = batched_masked_wiki.shape[1]\n",
    "for epoch in range(3):\n",
    "    print(\"epoch\", epoch)\n",
    "    for batch_num in range(num_batches):\n",
    "        adam.zero_grad()\n",
    "        b = batched_masked_wiki[batch_num].cuda()\n",
    "        l = batched_wiki[batch_num].cuda()\n",
    "        print(tiny_bert(b).shape)\n",
    "        out = tiny_bert(b)\n",
    "        \n",
    "        is_mask_token = (b == 103)\n",
    "        masked_tokens = l.masked_select(is_mask_token)\n",
    "        predictions = einops.rearrange(out.masked_select(is_mask_token.unsqueeze(-1)), \"(h w) -> h w\", w= vocab_size)\n",
    "        actual = t.zeros_like(predictions)\n",
    "        for i in range(masked_tokens.shape[0]):\n",
    "            actual[i, masked_tokens[i].int()] = 1\n",
    "        print(predictions.shape, actual.shape)\n",
    "        out_loss = loss(predictions, actual)\n",
    "        out_loss.backward()\n",
    "        adam.step()\n",
    "        if batch_num % 20 == 0:\n",
    "            print(\"batch\", batch_num, \"loss\", out_loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
